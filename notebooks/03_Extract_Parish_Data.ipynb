{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Extract Parish Data\n",
    "\n",
    "This notebook extracts detailed parish information from discovered parish directory pages.\n",
    "\n",
    "**What this does**:\n",
    "- Sets up the complete environment (no separate setup notebook needed)\n",
    "- Detects website patterns and selects optimal extraction strategies\n",
    "- Extracts comprehensive parish data including addresses, contacts, and schedules\n",
    "- Handles multiple website platforms (eCatholic, SquareSpace, WordPress, etc.)\n",
    "- Saves extracted parish data to Supabase database\n",
    "\n",
    "**Prerequisites**: \n",
    "- Run `01_Build_Dioceses_Database.ipynb` to populate dioceses\n",
    "- Run `02_Find_Parish_Directories.ipynb` to discover directory URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "complete_setup"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Complete Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 Setting up USCCB Parish Extraction Environment...\\n\")\n",
    "\n",
    "# Step 1: Clone repository if needed\n",
    "repo_path = '/content/usccb-parish-extraction'\n",
    "if not os.path.exists(repo_path):\n",
    "    print(\"📁 Cloning repository...\")\n",
    "    !git clone https://github.com/tomknightatl/usccb-parish-extraction.git\n",
    "    print(\"✅ Repository cloned\")\n",
    "else:\n",
    "    print(\"✅ Repository already exists\")\n",
    "    os.chdir(repo_path)\n",
    "    !git pull --quiet\n",
    "    print(\"✅ Repository updated\")\n",
    "\n",
    "# Step 2: Set working directory and Python path\n",
    "os.chdir(repo_path)\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "print(f\"📂 Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Step 3: Install required packages\n",
    "print(\"\\n📦 Installing packages...\")\n",
    "!pip install --quiet selenium==4.15.0 webdriver-manager==4.0.1\n",
    "!pip install --quiet beautifulsoup4==4.12.2 lxml\n",
    "!pip install --quiet google-generativeai==0.3.0 tenacity==8.2.3\n",
    "!pip install --quiet \"supabase>=2.15.0\"\n",
    "print(\"✅ Packages installed\")\n",
    "\n",
    "# Step 4: Import required modules\n",
    "print(\"\\n🧪 Testing imports...\")\n",
    "try:\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    print(\"✅ External packages imported\")\n",
    "    \n",
    "    from config.settings import setup_environment, set_config, get_config\n",
    "    from src.models import Diocese, Parish, ExtractionResult, SiteType\n",
    "    from src.utils.webdriver import setup_driver\n",
    "    from src.extractors import get_extractor\n",
    "    from src.utils.ai_analysis import detect_site_type\n",
    "    print(\"✅ Project modules imported\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"\\n🔧 Try restarting runtime and running this cell again\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Configure APIs\n",
    "print(\"\\n🔑 Configuring APIs...\")\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    supabase_url = userdata.get('SUPABASE_URL')\n",
    "    supabase_key = userdata.get('SUPABASE_KEY')\n",
    "    genai_key = userdata.get('GENAI_API_KEY_USCCB')\n",
    "    \n",
    "    config = setup_environment(\n",
    "        supabase_url=supabase_url,\n",
    "        supabase_key=supabase_key,\n",
    "        genai_api_key=genai_key,\n",
    "        max_dioceses=3  # Process 3 dioceses for testing\n",
    "    )\n",
    "    set_config(config)\n",
    "    \n",
    "    print(\"✅ Configuration complete\")\n",
    "    print(f\"   📊 Database: {'Connected' if config.supabase else 'Not connected'}\")\n",
    "    print(f\"   🤖 AI: {'Enabled' if config.genai_enabled else 'Mock mode'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Configuration error: {e}\")\n",
    "    print(\"\\n🔧 Make sure to add your API keys to Colab Secrets:\")\n",
    "    print(\"   • SUPABASE_URL\")\n",
    "    print(\"   • SUPABASE_KEY\")\n",
    "    print(\"   • GENAI_API_KEY_USCCB\")\n",
    "    config = None\n",
    "\n",
    "print(\"\\n🎉 Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extraction_functions"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Parish Extraction Functions\n",
    "\n",
    "def get_dioceses_with_directories(limit=None):\n",
    "    \"\"\"Get dioceses that have parish directory URLs and need parish extraction.\"\"\"\n",
    "    if not config or not config.supabase:\n",
    "        print(\"❌ No database connection\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Get dioceses with parish directory URLs\n",
    "        try:\n",
    "            response = config.supabase.table('DiocesesParishDirectory').select(\n",
    "                'diocese_url, parish_directory_url'\n",
    "            ).not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
    "            \n",
    "            diocese_directories = response.data or []\n",
    "        except:\n",
    "            print(\"❌ DiocesesParishDirectory table not found\")\n",
    "            print(\"\\n🔧 Run 02_Find_Parish_Directories.ipynb first\")\n",
    "            return []\n",
    "        \n",
    "        # Get diocese names\n",
    "        if diocese_directories:\n",
    "            diocese_urls = [item['diocese_url'] for item in diocese_directories]\n",
    "            \n",
    "            names_response = config.supabase.table('Dioceses').select(\n",
    "                'Website, Name'\n",
    "            ).in_('Website', diocese_urls).execute()\n",
    "            \n",
    "            url_to_name = {item['Website']: item['Name'] for item in (names_response.data or [])}\n",
    "            \n",
    "            # Combine data\n",
    "            dioceses_to_process = []\n",
    "            for item in diocese_directories:\n",
    "                diocese_url = item['diocese_url']\n",
    "                diocese_name = url_to_name.get(diocese_url, 'Unknown Diocese')\n",
    "                \n",
    "                dioceses_to_process.append({\n",
    "                    'name': diocese_name,\n",
    "                    'url': diocese_url,\n",
    "                    'parish_directory_url': item['parish_directory_url']\n",
    "                })\n",
    "            \n",
    "            if limit and len(dioceses_to_process) > limit:\n",
    "                import random\n",
    "                dioceses_to_process = random.sample(dioceses_to_process, limit)\n",
    "            \n",
    "            return dioceses_to_process\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching dioceses with directories: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_parishes_to_database(parishes, diocese_url, directory_url, extraction_method):\n",
    "    \"\"\"Save parishes to Supabase database with improved error handling\"\"\"\n",
    "    if not config or not config.supabase:\n",
    "        print(\"  📝 Would save parishes to database (no connection)\")\n",
    "        return len(parishes)\n",
    "    \n",
    "    saved_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Save in batches to avoid timeouts\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(parishes), batch_size):\n",
    "        batch = parishes[i:i + batch_size]\n",
    "        batch_data = []\n",
    "        \n",
    "        for parish in batch:\n",
    "            try:\n",
    "                parish_data = {\n",
    "                    'name': parish.name,\n",
    "                    'address': parish.address,\n",
    "                    'city': parish.city,\n",
    "                    'state': parish.state,\n",
    "                    'zip_code': parish.zip_code,\n",
    "                    'phone': parish.phone,\n",
    "                    'email': parish.email,\n",
    "                    'website': parish.website,\n",
    "                    'pastor': parish.pastor,\n",
    "                    'mass_times': parish.mass_times,\n",
    "                    'latitude': parish.latitude,\n",
    "                    'longitude': parish.longitude,\n",
    "                    'diocese_url': diocese_url,\n",
    "                    'directory_url': directory_url,\n",
    "                    'extraction_method': extraction_method,\n",
    "                    'confidence_score': parish.confidence,\n",
    "                    'extracted_at': datetime.now().isoformat()\n",
    "                }\n",
    "                batch_data.append(parish_data)\n",
    "            except Exception as e:\n",
    "                print(f\"    ⚠️ Error preparing parish data: {e}\")\n",
    "                failed_count += 1\n",
    "        \n",
    "        # Save the batch\n",
    "        if batch_data:\n",
    "            try:\n",
    "                response = config.supabase.table('Parishes').insert(batch_data).execute()\n",
    "                saved_count += len(batch_data)\n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Error saving batch to database: {e}\")\n",
    "                failed_count += len(batch_data)\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"    ⚠️ {failed_count} parishes failed to save\")\n",
    "    \n",
    "    return saved_count\n",
    "\n",
    "def extract_parishes_from_directory(diocese_info, driver):\n",
    "    \"\"\"Extract parishes from a single diocese directory page.\"\"\"\n",
    "    diocese_name = diocese_info['name']\n",
    "    diocese_url = diocese_info['url']\n",
    "    directory_url = diocese_info['parish_directory_url']\n",
    "    \n",
    "    print(f\"\\n🏛️ Extracting parishes from: {diocese_name}\")\n",
    "    print(f\"  📍 Diocese URL: {diocese_url}\")\n",
    "    print(f\"  📂 Directory URL: {directory_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the parish directory page\n",
    "        print(f\"  📥 Loading directory page...\")\n",
    "        driver.get(directory_url)\n",
    "        time.sleep(3)  # Give time for JS to load\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Detect site type\n",
    "        print(f\"  🔍 Detecting website pattern...\")\n",
    "        site_type = detect_site_type(soup, directory_url)\n",
    "        print(f\"    📊 Detected type: {site_type.value}\")\n",
    "        \n",
    "        # Get appropriate extractor\n",
    "        extractor = get_extractor(site_type.value)\n",
    "        print(f\"    🔧 Using extractor: {extractor.name}\")\n",
    "        \n",
    "        # Extract parishes\n",
    "        print(f\"  ⚙️ Extracting parish data...\")\n",
    "        parishes = extractor.extract(soup, directory_url, driver)\n",
    "        \n",
    "        print(f\"  ✅ Extracted {len(parishes)} parishes\")\n",
    "        \n",
    "        # Create extraction result\n",
    "        result = ExtractionResult(\n",
    "            diocese_name=diocese_name,\n",
    "            diocese_url=diocese_url,\n",
    "            directory_url=directory_url,\n",
    "            parishes=parishes,\n",
    "            site_type=site_type,\n",
    "            success=len(parishes) > 0\n",
    "        )\n",
    "        \n",
    "        # Save to database\n",
    "        if parishes:\n",
    "            print(f\"  💾 Saving parishes to database...\")\n",
    "            saved_count = save_parishes_to_database(\n",
    "                parishes, diocese_url, directory_url, site_type.value\n",
    "            )\n",
    "            result.saved_count = saved_count\n",
    "            print(f\"    📊 Saved {saved_count} parishes\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)[:100]\n",
    "        print(f\"  ❌ Error extracting from {diocese_name}: {error_msg}\")\n",
    "        \n",
    "        return ExtractionResult(\n",
    "            diocese_name=diocese_name,\n",
    "            diocese_url=diocese_url,\n",
    "            directory_url=directory_url,\n",
    "            parishes=[],\n",
    "            site_type=SiteType.GENERIC,\n",
    "            success=False,\n",
    "            errors=[error_msg]\n",
    "        )\n",
    "\n",
    "print(\"✅ Parish extraction functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_extraction"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Main Extraction Process\n",
    "\n",
    "# Set processing limit (you can change this)\n",
    "MAX_DIOCESES_TO_PROCESS = 3  # Process 3 dioceses as a test\n",
    "\n",
    "print(f\"🚀 Starting parish data extraction...\")\n",
    "print(f\"📊 Will process up to {MAX_DIOCESES_TO_PROCESS} dioceses\")\n",
    "\n",
    "# Get dioceses with directory URLs\n",
    "dioceses_to_process = get_dioceses_with_directories(limit=MAX_DIOCESES_TO_PROCESS)\n",
    "\n",
    "if not dioceses_to_process:\n",
    "    print(\"❌ No dioceses with parish directory URLs found\")\n",
    "    print(\"\\n🔧 Make sure you've run 02_Find_Parish_Directories.ipynb first\")\n",
    "else:\n",
    "    print(f\"📋 Found {len(dioceses_to_process)} dioceses with directory URLs\")\n",
    "    \n",
    "    # Show what we'll process\n",
    "    print(f\"\\n📋 Dioceses to process:\")\n",
    "    for i, diocese in enumerate(dioceses_to_process, 1):\n",
    "        print(f\"  {i}. {diocese['name']}\")\n",
    "        print(f\"     Directory: {diocese['parish_directory_url']}\")\n",
    "    \n",
    "    # Setup WebDriver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    if not driver:\n",
    "        print(\"❌ Failed to setup WebDriver\")\n",
    "    else:\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            for i, diocese_info in enumerate(dioceses_to_process, 1):\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"Processing diocese {i}/{len(dioceses_to_process)}\")\n",
    "                \n",
    "                result = extract_parishes_from_directory(diocese_info, driver)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Be respectful - pause between requests\n",
    "                if i < len(dioceses_to_process):\n",
    "                    print(f\"  ⏱️ Waiting {config.request_delay} seconds...\")\n",
    "                    time.sleep(config.request_delay)\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "            print(\"\\n🧹 WebDriver closed\")\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"📊 EXTRACTION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        total_parishes = sum(len(r.parishes) for r in results)\n",
    "        successful_extractions = sum(1 for r in results if r.success)\n",
    "        total_saved = sum(getattr(r, 'saved_count', 0) for r in results)\n",
    "        \n",
    "        print(f\"Total dioceses processed: {len(results)}\")\n",
    "        print(f\"Successful extractions: {successful_extractions}\")\n",
    "        print(f\"Total parishes found: {total_parishes}\")\n",
    "        print(f\"Total parishes saved: {total_saved}\")\n",
    "        \n",
    "        if successful_extractions > 0:\n",
    "            print(f\"Average parishes per diocese: {total_parishes/successful_extractions:.1f}\")\n",
    "            print(f\"Success rate: {successful_extractions/len(results)*100:.1f}%\")\n",
    "        \n",
    "        # Show site types detected\n",
    "        site_types = {}\n",
    "        for result in results:\n",
    "            if result.success:\n",
    "                site_type = result.site_type.value\n",
    "                site_types[site_type] = site_types.get(site_type, 0) + 1\n",
    "        \n",
    "        if site_types:\n",
    "            print(f\"\\n🔍 Website Types Detected:\")\n",
    "            for site_type, count in site_types.items():\n",
    "                print(f\"  {site_type.replace('_', ' ').title()}: {count} dioceses\")\n",
    "        \n",
    "        # Show detailed results\n",
    "        print(f\"\\n📋 Detailed Results:\")\n",
    "        for result in results:\n",
    "            status = \"✅\" if result.success else \"❌\"\n",
    "            parishes_info = f\"{len(result.parishes)} parishes\" if result.success else \"Failed\"\n",
    "            saved_info = f\" ({getattr(result, 'saved_count', 0)} saved)\" if getattr(result, 'saved_count', 0) > 0 else \"\"\n",
    "            \n",
    "            print(f\"  {status} {result.diocese_name}: {parishes_info}{saved_info}\")\n",
    "            print(f\"      Site Type: {result.site_type.value}\")\n",
    "            print(f\"      Directory: {result.directory_url}\")\n",
    "            \n",
    "            if result.errors:\n",
    "                for error in result.errors:\n",
    "                    print(f\"      Error: {error}\")\n",
    "            \n",
    "        # Show sample parishes\n",
    "        all_parishes = []\n",
    "        for result in results:\n",
    "            all_parishes.extend(result.parishes)\n",
    "        \n",
    "        if all_parishes:\n",
    "            print(f\"\\n🏛️ Sample Parishes Extracted:\")\n",
    "            for i, parish in enumerate(all_parishes[:10], 1):\n",
    "                print(f\"  {i}. {parish.name}\")\n",
    "                if parish.city:\n",
    "                    print(f\"     📍 {parish.city}\")\n",
    "                if parish.phone:\n",
    "                    print(f\"     📞 {parish.phone}\")\n",
    "                if parish.website:\n",
    "                    print(f\"     🌐 {parish.website}\")\n",
    "            \n",
    "            if len(all_parishes) > 10:\n",
    "                print(f\"     ... and {len(all_parishes) - 10} more parishes\")\n",
    "        \n",
    "        # Save detailed results to file\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f'parish_extraction_results_{timestamp}.json'\n",
    "            \n",
    "            # Convert results to serializable format\n",
    "            serializable_results = []\n",
    "            for result in results:\n",
    "                serializable_results.append({\n",
    "                    'diocese_name': result.diocese_name,\n",
    "                    'diocese_url': result.diocese_url,\n",
    "                    'directory_url': result.directory_url,\n",
    "                    'parish_count': len(result.parishes),\n",
    "                    'site_type': result.site_type.value,\n",
    "                    'success': result.success,\n",
    "                    'saved_count': getattr(result, 'saved_count', 0),\n",
    "                    'errors': result.errors,\n",
    "                    'parishes': [\n",
    "                        {\n",
    "                            'name': p.name,\n",
    "                            'city': p.city,\n",
    "                            'address': p.address,\n",
    "                            'phone': p.phone,\n",
    "                            'website': p.website,\n",
    "                            'confidence': p.confidence\n",
    "                        }\n",
    "                        for p in result.parishes\n",
    "                    ]\n",
    "                })\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"💾 Detailed results saved to: {filename}\")\n",
    "            \n",
    "            # Download file in Colab\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(f\"⬇️ Results file downloaded\")\n",
    "            except ImportError:\n",
    "                print(f\"📁 Results saved locally\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving results: {e}\")\n",
    "        \n",
    "        print(\"\\n🎉 Parish data extraction complete!\")\n",
    "        print(\"\\n📊 Check your Supabase database for the extracted parish data!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
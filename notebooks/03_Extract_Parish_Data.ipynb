{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Extract Parish Data\n",
    "\n",
    "This notebook extracts detailed parish information from discovered parish directory pages.\n",
    "\n",
    "**Prerequisites**: \n",
    "1. Run `00_Colab_Setup.ipynb` first\n",
    "2. Run `01_Build_Dioceses_Database.ipynb` to populate dioceses\n",
    "3. Run `02_Find_Parish_Directories.ipynb` to discover directory URLs\n",
    "\n",
    "**What this does**:\n",
    "- Detects website patterns and selects optimal extraction strategies\n",
    "- Extracts comprehensive parish data including addresses, contacts, and schedules\n",
    "- Handles multiple website platforms (eCatholic, SquareSpace, WordPress, etc.)\n",
    "- Saves extracted parish data to Supabase database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_environment"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup Environment and Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure we're in the correct directory and set up Python path\n",
    "repo_path = '/content/usccb-parish-extraction'\n",
    "\n",
    "if not os.path.exists(repo_path):\n",
    "    print(\"‚ùå Repository not found!\")\n",
    "    print(\"Please run 00_Colab_Setup.ipynb first to clone the repository.\")\n",
    "    raise FileNotFoundError(\"Repository not found\")\n",
    "\n",
    "# Change to repository directory and add to Python path\n",
    "os.chdir(repo_path)\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "print(\"üêç Python path configured\")\n",
    "\n",
    "# Import required modules\n",
    "try:\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    from config.settings import get_config\n",
    "    from src.pipeline import ParishExtractionPipeline\n",
    "    from src.models import Diocese, Parish, ExtractionResult, SiteType\n",
    "    from src.utils.webdriver import setup_driver\n",
    "    from src.utils.database import save_parishes_to_database, get_dioceses_to_process\n",
    "    from src.extractors import get_extractor\n",
    "    from src.utils.ai_analysis import detect_site_type\n",
    "    \n",
    "    print(\"‚úÖ All modules imported successfully\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting:\")\n",
    "    print(\"1. Make sure you've run 00_Colab_Setup.ipynb completely\")\n",
    "    print(\"2. If you restarted the runtime, re-run the setup notebook\")\n",
    "    print(\"3. Check that all required packages are installed\")\n",
    "    raise\n",
    "\n",
    "# Get configuration\n",
    "try:\n",
    "    config = get_config()\n",
    "    print(\"‚úÖ Configuration loaded successfully\")\n",
    "    print(f\"üìä Database: {'Connected' if config.supabase else 'Not connected'}\")\n",
    "    print(f\"ü§ñ AI: {'Enabled' if config.genai_enabled else 'Mock mode'}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(\"\\nüîß Please run 00_Colab_Setup.ipynb first to configure your environment.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extraction_functions"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Parish Extraction Functions\n",
    "\n",
    "def get_dioceses_with_directories(limit=None):\n",
    "    \"\"\"Get dioceses that have parish directory URLs and need parish extraction.\"\"\"\n",
    "    if not config.supabase:\n",
    "        print(\"‚ùå No database connection\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Get dioceses with parish directory URLs\n",
    "        response = config.supabase.table('DiocesesParishDirectory').select(\n",
    "            'diocese_url, parish_directory_url'\n",
    "        ).not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
    "        \n",
    "        diocese_directories = response.data or []\n",
    "        \n",
    "        # Get diocese names\n",
    "        if diocese_directories:\n",
    "            diocese_urls = [item['diocese_url'] for item in diocese_directories]\n",
    "            \n",
    "            names_response = config.supabase.table('Dioceses').select(\n",
    "                'Website, Name'\n",
    "            ).in_('Website', diocese_urls).execute()\n",
    "            \n",
    "            url_to_name = {item['Website']: item['Name'] for item in (names_response.data or [])}\n",
    "            \n",
    "            # Combine data\n",
    "            dioceses_to_process = []\n",
    "            for item in diocese_directories:\n",
    "                diocese_url = item['diocese_url']\n",
    "                diocese_name = url_to_name.get(diocese_url, 'Unknown Diocese')\n",
    "                \n",
    "                dioceses_to_process.append({\n",
    "                    'name': diocese_name,\n",
    "                    'url': diocese_url,\n",
    "                    'parish_directory_url': item['parish_directory_url']\n",
    "                })\n",
    "            \n",
    "            if limit and len(dioceses_to_process) > limit:\n",
    "                import random\n",
    "                dioceses_to_process = random.sample(dioceses_to_process, limit)\n",
    "            \n",
    "            return dioceses_to_process\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching dioceses with directories: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_parishes_from_directory(diocese_info, driver):\n",
    "    \"\"\"Extract parishes from a single diocese directory page.\"\"\"\n",
    "    diocese_name = diocese_info['name']\n",
    "    diocese_url = diocese_info['url']\n",
    "    directory_url = diocese_info['parish_directory_url']\n",
    "    \n",
    "    print(f\"\\nüèõÔ∏è Extracting parishes from: {diocese_name}\")\n",
    "    print(f\"  üìç Diocese URL: {diocese_url}\")\n",
    "    print(f\"  üìÇ Directory URL: {directory_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the parish directory page\n",
    "        print(f\"  üì• Loading directory page...\")\n",
    "        driver.get(directory_url)\n",
    "        time.sleep(3)  # Give time for JS to load\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Detect site type\n",
    "        print(f\"  üîç Detecting website pattern...\")\n",
    "        site_type = detect_site_type(soup, directory_url)\n",
    "        print(f\"    üìä Detected type: {site_type.value}\")\n",
    "        \n",
    "        # Get appropriate extractor\n",
    "        extractor = get_extractor(site_type.value)\n",
    "        print(f\"    üîß Using extractor: {extractor.name}\")\n",
    "        \n",
    "        # Extract parishes\n",
    "        print(f\"  ‚öôÔ∏è Extracting parish data...\")\n",
    "        parishes = extractor.extract(soup, directory_url, driver)\n",
    "        \n",
    "        print(f\"  ‚úÖ Extracted {len(parishes)} parishes\")\n",
    "        \n",
    "        # Create extraction result\n",
    "        result = ExtractionResult(\n",
    "            diocese_name=diocese_name,\n",
    "            diocese_url=diocese_url,\n",
    "            directory_url=directory_url,\n",
    "            parishes=parishes,\n",
    "            site_type=site_type,\n",
    "            success=len(parishes) > 0\n",
    "        )\n",
    "        \n",
    "        # Save to database\n",
    "        if parishes:\n",
    "            print(f\"  üíæ Saving parishes to database...\")\n",
    "            saved_count = save_parishes_to_database(\n",
    "                parishes, diocese_url, directory_url, site_type.value\n",
    "            )\n",
    "            result.saved_count = saved_count\n",
    "            print(f\"    üìä Saved {saved_count} parishes\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)[:100]\n",
    "        print(f\"  ‚ùå Error extracting from {diocese_name}: {error_msg}\")\n",
    "        \n",
    "        return ExtractionResult(\n",
    "            diocese_name=diocese_name,\n",
    "            diocese_url=diocese_url,\n",
    "            directory_url=directory_url,\n",
    "            parishes=[],\n",
    "            site_type=SiteType.GENERIC,\n",
    "            success=False,\n",
    "            errors=[error_msg]\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Parish extraction functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_extraction"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Main Extraction Process\n",
    "\n",
    "# Set processing limit (you can change this)\n",
    "MAX_DIOCESES_TO_PROCESS = 3  # Process 3 dioceses as a test\n",
    "\n",
    "print(f\"üöÄ Starting parish data extraction...\")\n",
    "print(f\"üìä Will process up to {MAX_DIOCESES_TO_PROCESS} dioceses\")\n",
    "\n",
    "# Get dioceses with directory URLs\n",
    "dioceses_to_process = get_dioceses_with_directories(limit=MAX_DIOCESES_TO_PROCESS)\n",
    "\n",
    "if not dioceses_to_process:\n",
    "    print(\"‚ùå No dioceses with parish directory URLs found\")\n",
    "    print(\"\\nüîß Make sure you've run 02_Find_Parish_Directories.ipynb first\")\n",
    "else:\n",
    "    print(f\"üìã Found {len(dioceses_to_process)} dioceses with directory URLs\")\n",
    "    \n",
    "    # Show what we'll process\n",
    "    print(f\"\\nüìã Dioceses to process:\")\n",
    "    for i, diocese in enumerate(dioceses_to_process, 1):\n",
    "        print(f\"  {i}. {diocese['name']}\")\n",
    "        print(f\"     Directory: {diocese['parish_directory_url']}\")\n",
    "    \n",
    "    # Setup WebDriver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    if not driver:\n",
    "        print(\"‚ùå Failed to setup WebDriver\")\n",
    "    else:\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            for i, diocese_info in enumerate(dioceses_to_process, 1):\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"Processing diocese {i}/{len(dioceses_to_process)}\")\n",
    "                \n",
    "                result = extract_parishes_from_directory(diocese_info, driver)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Be respectful - pause between requests\n",
    "                if i < len(dioceses_to_process):\n",
    "                    print(f\"  ‚è±Ô∏è Waiting {config.request_delay} seconds...\")\n",
    "                    time.sleep(config.request_delay)\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "            print(\"\\nüßπ WebDriver closed\")\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìä EXTRACTION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        total_parishes = sum(len(r.parishes) for r in results)\n",
    "        successful_extractions = sum(1 for r in results if r.success)\n",
    "        total_saved = sum(r.saved_count for r in results)\n",
    "        \n",
    "        print(f\"Total dioceses processed: {len(results)}\")\n",
    "        print(f\"Successful extractions: {successful_extractions}\")\n",
    "        print(f\"Total parishes found: {total_parishes}\")\n",
    "        print(f\"Total parishes saved: {total_saved}\")\n",
    "        \n",
    "        if successful_extractions > 0:\n",
    "            print(f\"Average parishes per diocese: {total_parishes/successful_extractions:.1f}\")\n",
    "            print(f\"Success rate: {successful_extractions/len(results)*100:.1f}%\")\n",
    "        \n",
    "        # Show site types detected\n",
    "        site_types = {}\n",
    "        for result in results:\n",
    "            if result.success:\n",
    "                site_type = result.site_type.value\n",
    "                site_types[site_type] = site_types.get(site_type, 0) + 1\n",
    "        \n",
    "        if site_types:\n",
    "            print(f\"\\nüîç Website Types Detected:\")\n",
    "            for site_type, count in site_types.items():\n",
    "                print(f\"  {site_type.replace('_', ' ').title()}: {count} dioceses\")\n",
    "        \n",
    "        # Show detailed results\n",
    "        print(f\"\\nüìã Detailed Results:\")\n",
    "        for result in results:\n",
    "            status = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "            parishes_info = f\"{len(result.parishes)} parishes\" if result.success else \"Failed\"\n",
    "            saved_info = f\" ({result.saved_count} saved)\" if result.saved_count > 0 else \"\"\n",
    "            \n",
    "            print(f\"  {status} {result.diocese_name}: {parishes_info}{saved_info}\")\n",
    "            print(f\"      Site Type: {result.site_type.value}\")\n",
    "            print(f\"      Directory: {result.directory_url}\")\n",
    "            \n",
    "            if result.errors:\n",
    "                for error in result.errors:\n",
    "                    print(f\"      Error: {error}\")\n",
    "            \n",
    "        # Show sample parishes\n",
    "        all_parishes = []\n",
    "        for result in results:\n",
    "            all_parishes.extend(result.parishes)\n",
    "        \n",
    "        if all_parishes:\n",
    "            print(f\"\\nüèõÔ∏è Sample Parishes Extracted:\")\n",
    "            for i, parish in enumerate(all_parishes[:10], 1):\n",
    "                print(f\"  {i}. {parish.name}\")\n",
    "                if parish.city:\n",
    "                    print(f\"     üìç {parish.city}\")\n",
    "                if parish.phone:\n",
    "                    print(f\"     üìû {parish.phone}\")\n",
    "                if parish.website:\n",
    "                    print(f\"     üåê {parish.website}\")\n",
    "            \n",
    "            if len(all_parishes) > 10:\n",
    "                print(f\"     ... and {len(all_parishes) - 10} more parishes\")\n",
    "        \n",
    "        # Save detailed results to file\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f'parish_extraction_results_{timestamp}.json'\n",
    "            \n",
    "            # Convert results to serializable format\n",
    "            serializable_results = []\n",
    "            for result in results:\n",
    "                serializable_results.append({\n",
    "                    'diocese_name': result.diocese_name,\n",
    "                    'diocese_url': result.diocese_url,\n",
    "                    'directory_url': result.directory_url,\n",
    "                    'parish_count': result.parish_count,\n",
    "                    'site_type': result.site_type.value,\n",
    "                    'success': result.success,\n",
    "                    'saved_count': result.saved_count,\n",
    "                    'errors': result.errors,\n",
    "                    'parishes': [\n",
    "                        {\n",
    "                            'name': p.name,\n",
    "                            'city': p.city,\n",
    "                            'address': p.address,\n",
    "                            'phone': p.phone,\n",
    "                            'website': p.website,\n",
    "                            'confidence': p.confidence\n",
    "                        }\n",
    "                        for p in result.parishes\n",
    "                    ]\n",
    "                })\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"üíæ Detailed results saved to: {filename}\")\n",
    "            \n",
    "            # Download file in Colab\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(f\"‚¨áÔ∏è Results file downloaded\")\n",
    "            except ImportError:\n",
    "                print(f\"üìÅ Results saved locally\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "completion_summary"
   },
   "source": [
    "## üéâ Parish Data Extraction Complete!\n",
    "\n",
    "If you see successful extractions above, you now have detailed parish data in your database!\n",
    "\n",
    "### ‚úÖ What You've Accomplished:\n",
    "- ‚úÖ Detected website patterns and selected optimal extraction strategies\n",
    "- ‚úÖ Extracted comprehensive parish data including names, addresses, and contact info\n",
    "- ‚úÖ Handled multiple website platforms automatically\n",
    "- ‚úÖ Saved parish data to your Supabase database\n",
    "- ‚úÖ Generated detailed extraction statistics and quality metrics\n",
    "\n",
    "### üìä Your Data Now Includes:\n",
    "- **Parish Names and Locations**: Complete identification information\n",
    "- **Contact Information**: Phone numbers and websites where available\n",
    "- **Geographic Data**: Addresses and coordinates for mapping\n",
    "- **Quality Metrics**: Confidence scores and extraction methods\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Scale Up Processing**\n",
    "   - Increase `MAX_DIOCESES_TO_PROCESS` to extract from more dioceses\n",
    "   - The system will automatically skip already-processed dioceses\n",
    "\n",
    "2. **Analyze Your Data**\n",
    "   - Query your Supabase `Parishes` table to explore the extracted data\n",
    "   - Use the quality metrics to identify the most reliable extractions\n",
    "\n",
    "3. **Advanced Features**\n",
    "   - Run the other specialized notebooks for specific data extraction\n",
    "   - Explore the detailed extraction results JSON file\n",
    "\n",
    "### üõ†Ô∏è Troubleshooting:\n",
    "\n",
    "**If no parishes were extracted:**\n",
    "- Check that you've run `02_Find_Parish_Directories.ipynb` first\n",
    "- Verify your Supabase database has data in the `DiocesesParishDirectory` table\n",
    "- Some diocese websites may have changed or be temporarily unavailable\n",
    "\n",
    "**If extraction failed for specific dioceses:**\n",
    "- The system handles different website types, but some may require custom handling\n",
    "- Check the error messages for specific issues\n",
    "- The system will retry failed dioceses on subsequent runs\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Tip**: The extraction system is designed to be re-runnable. You can safely run this notebook multiple times to process additional dioceses or retry failed extractions!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Build Dioceses Database\n",
    "\n",
    "This notebook scrapes the USCCB website to build the initial dioceses database.\n",
    "\n",
    "**Prerequisites**: Run `00_Colab_Setup.ipynb` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from config.settings import get_config\n",
    "from src.utils.webdriver import setup_driver, load_page, clean_text\n",
    "\n",
    "# Get configuration\n",
    "try:\n",
    "    config = get_config()\n",
    "    print(\"‚úÖ Configuration loaded\")\n",
    "except RuntimeError:\n",
    "    print(\"‚ùå Please run 00_Colab_Setup.ipynb first\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scrape_usccb"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Scrape USCCB Dioceses Page\n",
    "def scrape_dioceses_from_usccb():\n",
    "    \"\"\"Scrape dioceses information from USCCB website\"\"\"\n",
    "    url = \"https://www.usccb.org/about/bishops-and-dioceses/all-dioceses\"\n",
    "    print(f\"Scraping dioceses from: {url}\")\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        soup = load_page(driver, url)\n",
    "        print(\"‚úÖ Page loaded successfully\")\n",
    "        \n",
    "        # Find diocese containers\n",
    "        diocese_containers = soup.find_all('div', class_='views-row')\n",
    "        print(f\"Found {len(diocese_containers)} potential diocese containers\")\n",
    "        \n",
    "        dioceses = []\n",
    "        \n",
    "        for i, container in enumerate(diocese_containers):\n",
    "            diocese_data = extract_diocese_info(container)\n",
    "            if diocese_data:\n",
    "                dioceses.append(diocese_data)\n",
    "                if len(dioceses) % 10 == 0:\n",
    "                    print(f\"Processed {len(dioceses)} dioceses...\")\n",
    "        \n",
    "        print(f\"‚úÖ Successfully extracted {len(dioceses)} dioceses\")\n",
    "        return dioceses\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def extract_diocese_info(container):\n",
    "    \"\"\"Extract diocese information from a container element\"\"\"\n",
    "    try:\n",
    "        da_wrap = container.find('div', class_='da-wrap')\n",
    "        if not da_wrap:\n",
    "            return None\n",
    "        \n",
    "        # Extract name\n",
    "        name_div = da_wrap.find('div', class_='da-title')\n",
    "        if not name_div:\n",
    "            return None\n",
    "        name = clean_text(name_div.get_text())\n",
    "        \n",
    "        # Extract address\n",
    "        address_div = da_wrap.find('div', class_='da-address')\n",
    "        address_parts = []\n",
    "        if address_div:\n",
    "            for div in address_div.find_all('div', recursive=False):\n",
    "                text = clean_text(div.get_text())\n",
    "                if text:\n",
    "                    address_parts.append(text)\n",
    "        \n",
    "        address = \", \".join(address_parts) if address_parts else None\n",
    "        \n",
    "        # Extract website\n",
    "        website_div = da_wrap.find('div', class_='site')\n",
    "        website = None\n",
    "        if website_div:\n",
    "            link = website_div.find('a')\n",
    "            if link:\n",
    "                website = link.get('href')\n",
    "        \n",
    "        if name and len(name) > 2:\n",
    "            return {\n",
    "                'Name': name,\n",
    "                'Address': address,\n",
    "                'Website': website,\n",
    "                'extracted_at': datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting diocese info: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run the scraping\n",
    "dioceses_data = scrape_dioceses_from_usccb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataframe"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Create DataFrame and Display Results\n",
    "if dioceses_data:\n",
    "    df = pd.DataFrame(dioceses_data)\n",
    "    \n",
    "    print(f\"üìä Created DataFrame with {len(df)} dioceses\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst 5 dioceses:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Check for missing websites\n",
    "    missing_websites = df['Website'].isna().sum()\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"Total dioceses: {len(df)}\")\n",
    "    print(f\"With websites: {len(df) - missing_websites}\")\n",
    "    print(f\"Missing websites: {missing_websites}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No dioceses data extracted\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_database"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Save to Database\n",
    "if not df.empty and config.supabase:\n",
    "    print(\"üíæ Saving dioceses to database...\")\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    try:\n",
    "        # Insert data in batches to avoid timeouts\n",
    "        batch_size = 20\n",
    "        total_inserted = 0\n",
    "        \n",
    "        for i in range(0, len(records), batch_size):\n",
    "            batch = records[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                response = config.supabase.table('Dioceses').insert(batch).execute()\n",
    "                \n",
    "                if hasattr(response, 'error') and response.error:\n",
    "                    print(f\"‚ùå Database error for batch {i//batch_size + 1}: {response.error}\")\n",
    "                else:\n",
    "                    total_inserted += len(batch)\n",
    "                    print(f\"‚úÖ Inserted batch {i//batch_size + 1}: {len(batch)} dioceses\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error inserting batch {i//batch_size + 1}: {e}\")\n",
    "            \n",
    "            # Small delay between batches\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(f\"\\nüìä Final Results:\")\n",
    "        print(f\"Total dioceses extracted: {len(df)}\")\n",
    "        print(f\"Successfully saved to database: {total_inserted}\")\n",
    "        \n",
    "        if total_inserted > 0:\n",
    "            print(f\"‚úÖ Dioceses database built successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database operation failed: {e}\")\n",
    "\n",
    "elif df.empty:\n",
    "    print(\"‚ùå No data to save\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Database not configured - data not saved\")\n",
    "    print(\"You can still use the extracted data from the DataFrame 'df'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_csv"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Export to CSV (Optional)\n",
    "if not df.empty:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'dioceses_extracted_{timestamp}.csv'\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"üìÅ Data exported to: {filename}\")\n",
    "        \n",
    "        # Download file in Colab\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(filename)\n",
    "            print(f\"‚¨áÔ∏è File downloaded\")\n",
    "        except ImportError:\n",
    "            print(f\"üìÅ File saved locally: {filename}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to export\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Find Parish Directories\n",
    "\n",
    "This notebook discovers parish directory URLs on diocesan websites using AI-powered analysis.\n",
    "\n",
    "**What this does**:\n",
    "- Sets up the complete environment (no separate setup notebook needed)\n",
    "- Analyzes diocese websites to find parish directory pages\n",
    "- Uses Google Gemini AI for intelligent link classification\n",
    "- Provides fallback search using mock responses\n",
    "- Saves discovered directory URLs to Supabase database\n",
    "\n",
    "**Prerequisites**: You should run `01_Build_Dioceses_Database.ipynb` first to populate dioceses data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "complete_setup"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Complete Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Setting up USCCB Parish Extraction Environment...\\n\")\n",
    "\n",
    "# Step 1: Clone repository if needed\n",
    "repo_path = '/content/usccb-parish-extraction'\n",
    "if not os.path.exists(repo_path):\n",
    "    print(\"üìÅ Cloning repository...\")\n",
    "    !git clone https://github.com/tomknightatl/usccb-parish-extraction.git\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "    os.chdir(repo_path)\n",
    "    !git pull --quiet\n",
    "    print(\"‚úÖ Repository updated\")\n",
    "\n",
    "# Step 2: Set working directory and Python path\n",
    "os.chdir(repo_path)\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Step 3: Install required packages\n",
    "print(\"\\nüì¶ Installing packages...\")\n",
    "!pip install --quiet selenium==4.15.0 webdriver-manager==4.0.1\n",
    "!pip install --quiet beautifulsoup4==4.12.2 lxml\n",
    "!pip install --quiet google-generativeai==0.3.0 tenacity==8.2.3\n",
    "!pip install --quiet \"supabase>=2.15.0\"\n",
    "print(\"‚úÖ Packages installed\")\n",
    "\n",
    "# Step 4: Import required modules\n",
    "print(\"\\nüß™ Testing imports...\")\n",
    "try:\n",
    "    import time\n",
    "    import re\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "    from datetime import datetime\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import google.generativeai as genai\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "    from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "    print(\"‚úÖ External packages imported\")\n",
    "    \n",
    "    from config.settings import setup_environment, set_config, get_config\n",
    "    from src.utils.webdriver import setup_driver, load_page, clean_text\n",
    "    from src.utils.ai_analysis import analyze_with_ai\n",
    "    print(\"‚úÖ Project modules imported\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nüîß Try restarting runtime and running this cell again\")\n",
    "    raise\n",
    "\n",
    "# Step 5: Configure APIs\n",
    "print(\"\\nüîë Configuring APIs...\")\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    supabase_url = userdata.get('SUPABASE_URL')\n",
    "    supabase_key = userdata.get('SUPABASE_KEY')\n",
    "    genai_key = userdata.get('GENAI_API_KEY_USCCB')\n",
    "    \n",
    "    config = setup_environment(\n",
    "        supabase_url=supabase_url,\n",
    "        supabase_key=supabase_key,\n",
    "        genai_api_key=genai_key,\n",
    "        max_dioceses=5  # Process 5 dioceses for testing\n",
    "    )\n",
    "    set_config(config)\n",
    "    \n",
    "    print(\"‚úÖ Configuration complete\")\n",
    "    print(f\"   üìä Database: {'Connected' if config.supabase else 'Not connected'}\")\n",
    "    print(f\"   ü§ñ AI: {'Enabled' if config.genai_enabled else 'Mock mode'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(\"\\nüîß Make sure to add your API keys to Colab Secrets:\")\n",
    "    print(\"   ‚Ä¢ SUPABASE_URL\")\n",
    "    print(\"   ‚Ä¢ SUPABASE_KEY\")\n",
    "    print(\"   ‚Ä¢ GENAI_API_KEY_USCCB\")\n",
    "    config = None\n",
    "\n",
    "print(\"\\nüéâ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parish_directory_functions"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Parish Directory Discovery Functions\n",
    "\n",
    "def normalize_url_join(base_url, relative_url):
    """Properly joins URLs while avoiding double slashes."""
    if base_url.endswith('/') and relative_url.startswith('/'):
        base_url = base_url.rstrip('/')
    return urljoin(base_url, relative_url)

def get_surrounding_text(element, max_length=200):
    """Extracts context text from the parent element of a link."""
    if element and element.parent:
        parent_text = element.parent.get_text(separator=' ', strip=True)
        return parent_text[:max_length] + ('...' if len(parent_text) > max_length else '')
    return ''

def find_candidate_urls(soup, base_url):
    """Scans a webpage for potential parish directory links."""
    candidate_links = []
    processed_hrefs = set()

    # Keywords likely to appear in parish directory links
    parish_link_keywords = [
        'Churches', 'Directory of Parishes', 'Parishes', 'parishfinder', 'Parish Finder',
        'Find a Parish', 'Locations', 'Our Parishes', 'Parish Listings', 'Find a Church',
        'Church Directory', 'Faith Communities', 'Find Mass Times', 'Our Churches',
        'Search Parishes', 'Parish Map', 'Mass Schedule', 'Sacraments', 'Worship'
    ]
    
    # URL path patterns for parish directories
    url_patterns = [
        r'parishes', r'directory', r'locations', r'churches',
        r'parish-finder', r'findachurch', r'parishsearch', r'parishdirectory',
        r'find-a-church', r'church-directory', r'parish-listings', r'parish-map',
        r'mass-times', r'sacraments', r'search', r'worship', r'finder'
    ]

    all_links = soup.find_all('a', href=True)

    for link_tag in all_links:
        href = link_tag['href']
        if not href or href.startswith('#') or href.lower().startswith('javascript:') or href.lower().startswith('mailto:'):
            continue

        abs_href = normalize_url_join(base_url, href)
        if not abs_href.startswith('http') or abs_href in processed_hrefs:
            continue

        link_text = link_tag.get_text(strip=True)
        surrounding_text = get_surrounding_text(link_tag)
        parsed_href_path = urlparse(abs_href).path.lower()

        # Check for matches based on keywords or URL patterns
        text_match = any(keyword.lower() in link_text.lower() or keyword.lower() in surrounding_text.lower() 
                        for keyword in parish_link_keywords)
        pattern_match = any(re.search(pattern, parsed_href_path, re.IGNORECASE) 
                           for pattern in url_patterns)

        if text_match or pattern_match:
            candidate_links.append({
                'text': link_text,
                'href': abs_href,
                'surrounding_text': surrounding_text
            })
            processed_hrefs.add(abs_href)

    return candidate_links

def analyze_links_with_ai(candidate_links, diocese_name=None):
    """Analyzes candidate links using AI to find the best parish directory URL."""
    best_link = None
    highest_score = -1

    print(f"    ü§ñ Analyzing {len(candidate_links)} candidate links with AI...")

    for link_info in candidate_links:
        try:
            # Create analysis prompt
            link_context = f"Link text: '{link_info['text']}', URL: '{link_info['href']}', Context: '{link_info['surrounding_text'][:100]}'"
            
            # Use the AI analysis utility
            analysis = analyze_with_ai(link_context, "parish_directory")
            score = analysis.get('score', 0)

            print(f"      üìä '{link_info['text'][:30]}...' -> Score: {score}")

            if score >= config.ai_confidence_threshold and score > highest_score:
                highest_score = score
                best_link = link_info['href']
                
        except Exception as e:
            print(f"      ‚ùå Error analyzing link: {str(e)[:50]}...")
            continue

    return best_link

  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "database_functions"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Database Functions\n",
    "\n",
    "def get_dioceses_to_process(limit=None):\n",
    "    \"\"\"Get dioceses that need parish directory URL discovery.\"\"\"\n",
    "    if not config or not config.supabase:\n",
    "        print(\"‚ùå No database connection\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Get all dioceses\n",
    "        response = config.supabase.table('Dioceses').select('Website, Name').execute()\n",
    "        all_dioceses = response.data or []\n",
    "        \n",
    "        # Get dioceses that already have directory URLs\n",
    "        try:\n",
    "            processed_response = config.supabase.table('DiocesesParishDirectory').select(\n",
    "                'diocese_url'\n",
    "            ).not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
    "            \n",
    "            processed_urls = {item['diocese_url'] for item in (processed_response.data or [])}\n",
    "        except:\n",
    "            # Table might not exist yet, process all dioceses\n",
    "            processed_urls = set()\n",
    "        \n",
    "        # Filter to unprocessed dioceses\n",
    "        unprocessed = [\n",
    "            {'url': d['Website'], 'name': d['Name']}\n",
    "            for d in all_dioceses\n",
    "            if d.get('Website') and d['Website'] not in processed_urls\n",
    "        ]\n",
    "        \n",
    "        if limit and len(unprocessed) > limit:\n",
    "            import random\n",
    "            unprocessed = random.sample(unprocessed, limit)\n",
    "        \n",
    "        return unprocessed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching dioceses: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_directory_result(diocese_url, directory_url, success, method=\"ai_analysis\"):\n",
    "    \"\"\"Save parish directory discovery result to database.\"\"\"\n",
    "    if not config or not config.supabase:\n",
    "        print(f\"  üìù Would save: {diocese_url} -> {directory_url or 'Not Found'}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        data = {\n",
    "            'diocese_url': diocese_url,\n",
    "            'parish_directory_url': directory_url,\n",
    "            'found': 'Success' if success else 'Not Found',\n",
    "            'found_method': method,\n",
    "            'updated_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        config.supabase.table('DiocesesParishDirectory').upsert(data).execute()\n",
    "        print(f\"  üíæ Saved result to database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error saving to database: {e}\")\n",
    "\n",
    "def search_for_directory_link(diocese_name, diocese_website_url):\n",
    "    \"\"\"Uses mock search results as fallback to find parish directory links.\"\"\"\n",
    "    print(f\"    üîç Using search engine fallback for {diocese_name}...\")\n",
    "    \n",
    "    # Generate mock search results based on common patterns\n",
    "    print(f\"    üìù Using mock search results\")\n",
    "    mock_results = [\n",
    "        {\n",
    "            'link': normalize_url_join(diocese_website_url, '/parishes'),\n",
    "            'title': f\"Parishes - {diocese_name}\",\n",
    "            'snippet': f\"List of parishes in the Diocese of {diocese_name}. Find a parish near you.\"\n",
    "        },\n",
    "        {\n",
    "            'link': normalize_url_join(diocese_website_url, '/directory'),\n",
    "            'title': f\"Directory - {diocese_name}\",\n",
    "            'snippet': f\"Official directory of churches and schools for {diocese_name}.\"\n",
    "        },\n",
    "        {\n",
    "            'link': normalize_url_join(diocese_website_url, '/find-a-church'),\n",
    "            'title': f\"Find a Church - {diocese_name}\",\n",
    "            'snippet': f\"Search for a Catholic church in {diocese_name}. Mass times and locations.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return analyze_search_snippets_with_ai(mock_results, diocese_name)\n",
    "\n",
    "def analyze_search_snippets_with_ai(search_results, diocese_name):\n",
    "    \"\"\"Analyzes search result snippets to find the best parish directory URL.\"\"\"\n",
    "    best_link = None\n",
    "    highest_score = -1\n",
    "\n",
    "    print(f\"    ü§ñ Analyzing {len(search_results)} search snippets with AI...\")\n",
    "\n",
    "    for result in search_results:\n",
    "        try:\n",
    "            snippet_context = f\"Title: '{result.get('title', '')}', Snippet: '{result.get('snippet', '')}', URL: '{result.get('link', '')}'\"\n",
    "            \n",
    "            analysis = analyze_with_ai(snippet_context, \"parish_directory\")\n",
    "            score = analysis.get('score', 0)\n",
    "\n",
    "            print(f\"      üìä '{result.get('title', '')[:30]}...' -> Score: {score}\")\n",
    "\n",
    "            if score >= config.ai_confidence_threshold and score > highest_score:\n",
    "                highest_score = score\n",
    "                best_link = result.get('link')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Error analyzing snippet: {str(e)[:50]}...\")\n",
    "            continue\n",
    "\n",
    "    return best_link\n",
    "\n",
    "print(\"‚úÖ Database and search functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_processing"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Main Processing Loop\n",
    "\n",
    "def process_diocese_for_directory(diocese_info, driver):\n",
    "    \"\"\"Process a single diocese to find its parish directory URL.\"\"\"\n",
    "    diocese_url = diocese_info['url']\n",
    "    diocese_name = diocese_info['name']\n",
    "    \n",
    "    print(f\"\\nüèõÔ∏è Processing: {diocese_name}\")\n",
    "    print(f\"  üìç URL: {diocese_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the diocese website\n",
    "        print(f\"  üì• Loading website...\")\n",
    "        soup = load_page(driver, diocese_url)\n",
    "        \n",
    "        # Find candidate links\n",
    "        print(f\"  üîç Scanning for parish directory links...\")\n",
    "        candidate_links = find_candidate_urls(soup, diocese_url)\n",
    "        \n",
    "        if not candidate_links:\n",
    "            print(f\"  ‚ö†Ô∏è No candidate links found on main page\")\n",
    "            # Try search engine fallback\n",
    "            directory_url = search_for_directory_link(diocese_name, diocese_url)\n",
    "            method = \"search_engine_fallback\"\n",
    "        else:\n",
    "            print(f\"  üìã Found {len(candidate_links)} candidate links\")\n",
    "            # Analyze with AI\n",
    "            directory_url = analyze_links_with_ai(candidate_links, diocese_name)\n",
    "            method = \"ai_direct_analysis\"\n",
    "        \n",
    "        # Save result\n",
    "        success = directory_url is not None\n",
    "        if success:\n",
    "            print(f\"  ‚úÖ Found directory URL: {directory_url}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå No parish directory URL found\")\n",
    "        \n",
    "        save_directory_result(diocese_url, directory_url, success, method)\n",
    "        \n",
    "        return {\n",
    "            'diocese_name': diocese_name,\n",
    "            'diocese_url': diocese_url,\n",
    "            'directory_url': directory_url,\n",
    "            'success': success,\n",
    "            'method': method\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)[:100]\n",
    "        print(f\"  ‚ùå Error processing {diocese_name}: {error_msg}\")\n",
    "        \n",
    "        # Save error result\n",
    "        save_directory_result(diocese_url, None, False, f\"error: {error_msg}\")\n",
    "        \n",
    "        return {\n",
    "            'diocese_name': diocese_name,\n",
    "            'diocese_url': diocese_url,\n",
    "            'directory_url': None,\n",
    "            'success': False,\n",
    "            'method': 'error',\n",
    "            'error': error_msg\n",
    "        }\n",
    "\n",
    "# Set processing limit (you can change this)\n",
    "MAX_DIOCESES_TO_PROCESS = 5  # Process 5 dioceses as a test\n",
    "\n",
    "print(f\"üöÄ Starting parish directory discovery...\")\n",
    "print(f\"üìä Will process up to {MAX_DIOCESES_TO_PROCESS} dioceses\")\n",
    "\n",
    "# Get dioceses to process\n",
    "dioceses_to_scan = get_dioceses_to_process(limit=MAX_DIOCESES_TO_PROCESS)\n",
    "\n",
    "if not dioceses_to_scan:\n",
    "    print(\"‚ùå No dioceses found to process\")\n",
    "    print(\"\\nüîß Make sure you've run 01_Build_Dioceses_Database.ipynb first\")\n",
    "else:\n",
    "    print(f\"üìã Found {len(dioceses_to_scan)} dioceses to process\")\n",
    "    \n",
    "    # Setup WebDriver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    if not driver:\n",
    "        print(\"‚ùå Failed to setup WebDriver\")\n",
    "    else:\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            for i, diocese_info in enumerate(dioceses_to_scan, 1):\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Processing diocese {i}/{len(dioceses_to_scan)}\")\n",
    "                \n",
    "                result = process_diocese_for_directory(diocese_info, driver)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Be respectful - pause between requests\n",
    "                if i < len(dioceses_to_scan):\n",
    "                    print(f\"  ‚è±Ô∏è Waiting {config.request_delay} seconds...\")\n",
    "                    time.sleep(config.request_delay)\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "            print(\"\\nüßπ WebDriver closed\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        successful = sum(1 for r in results if r['success'])\n",
    "        failed = len(results) - successful\n",
    "        \n",
    "        print(f\"Total dioceses processed: {len(results)}\")\n",
    "        print(f\"Successfully found directories: {successful}\")\n",
    "        print(f\"Failed to find directories: {failed}\")\n",
    "        print(f\"Success rate: {successful/len(results)*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüìã Detailed Results:\")\n",
    "        for result in results:\n",
    "            status = \"‚úÖ\" if result['success'] else \"‚ùå\"\n",
    "            print(f\"  {status} {result['diocese_name']}\")\n",
    "            if result['success']:\n",
    "                print(f\"      Directory: {result['directory_url']}\")\n",
    "            print(f\"      Method: {result['method']}\")\n",
    "            if 'error' in result:\n",
    "                print(f\"      Error: {result['error']}\")\n",
    "            print()\n",
    "        \n",
    "        print(\"\\nüéâ Parish directory discovery complete!\")\n",
    "        print(\"\\nüìö Next step: Run 03_Extract_Parish_Data.ipynb to extract parish information\")"
   ]
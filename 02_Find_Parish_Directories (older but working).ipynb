{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomknightatl/usccb-parish-extraction/blob/main/02_Find_Parish_Directories%20(older%20but%20working).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "param_documentation_cell"
      },
      "source": [
        "# Notebook Configuration Parameters\n",
        "\n",
        "This notebook is designed to find parish directory URLs on diocesan websites. The first code cell below, labeled \"User-configurable parameters,\" contains all the necessary settings to control its behavior, including API key configurations, GitHub integration, and mocking controls.\n",
        "\n",
        "Please review and configure these parameters before running the notebook.\n",
        "\n",
        "## API Keys & Secrets\n",
        "\n",
        "For features that require external services (like Generative AI or Google Search), you'll need to provide API keys. These should be stored securely using Colab Secrets.\n",
        "\n",
        "### `GENAI_API_KEY`\n",
        "- **Purpose**: This API key is for Google Generative AI (e.g., Gemini models). It's used to analyze web page content and search result snippets to identify potential parish directory links.\n",
        "- **Configuration**:\n",
        "    1. Obtain your GenAI API key from Google AI Studio.\n",
        "    2. In Google Colab, go to \"Secrets\" (key icon in the left sidebar) and add a new secret named `GENAI_API_KEY_USCCB`. Paste your API key as the value.\n",
        "    3. In the \"User-configurable parameters\" cell, uncomment the line `# GENAI_API_KEY = GENAI_API_KEY_FROM_USERDATA` to use the key from Colab Secrets. Alternatively, you can directly assign your key string to `GENAI_API_KEY` in the cell, but using secrets is recommended.\n",
        "- **Default**: `None`. If no key is provided, GenAI-powered analysis will be disabled, and the system will rely on mock/basic analysis.\n",
        "- **Dependencies**: Required if you want to use live GenAI analysis. You'll also need to set `use_mock_genai_direct_page = False` and/or `use_mock_genai_snippet = False`.\n",
        "\n",
        "### `SEARCH_API_KEY` and `SEARCH_CX`\n",
        "- **Purpose**: These are for the Google Custom Search API. This API is used as a fallback mechanism to find parish directory links if direct analysis of the diocesan website doesn't yield clear results. `SEARCH_API_KEY` is your API key, and `SEARCH_CX` is your Programmable Search Engine ID.\n",
        "- **Configuration**:\n",
        "    1. Create a Programmable Search Engine on the Google Control Panel, configured to search diocesan websites. Note the Search Engine ID (`SEARCH_CX`).\n",
        "    2. Obtain your Google Cloud API Key enabled for the Custom Search API.\n",
        "    3. In Colab Secrets, add `SEARCH_API_KEY_USCCB` (with your API key) and `SEARCH_CX_USCCB` (with your Search Engine ID).\n",
        "    4. In the \"User-configurable parameters\" cell, uncomment the lines that assign these secrets to `SEARCH_API_KEY` and `SEARCH_CX`.\n",
        "- **Default**: `None` for both. If not set, the Google Custom Search fallback will be disabled, and the system will rely on mock/basic search.\n",
        "- **Dependencies**: Required for the search engine fallback feature. You'll also need to set `use_mock_search_engine = False`.\n",
        "\n",
        "## Mocking Controls\n",
        "\n",
        "These boolean flags allow you to run the notebook with mocked (simulated) API responses, which is useful for testing or when API keys are unavailable. Set them to `False` to use live APIs (requires corresponding API keys to be configured).\n",
        "\n",
        "### `use_mock_genai_direct_page`\n",
        "- **Purpose**: Controls whether GenAI analysis of links found directly on a webpage uses live API calls or mocked responses.\n",
        "- **Configuration**: Set to `True` for mock, `False` for live.\n",
        "- **Default**: `True`. GenAI analysis for direct page links will be mocked.\n",
        "- **Dependencies**: If set to `False`, a valid `GENAI_API_KEY` must be configured.\n",
        "\n",
        "### `use_mock_genai_snippet`\n",
        "- **Purpose**: Controls whether GenAI analysis of search result snippets (from Google Custom Search) uses live API calls or mocked responses.\n",
        "- **Configuration**: Set to `True` for mock, `False` for live.\n",
        "- **Default**: `True`. GenAI analysis for search snippets will be mocked.\n",
        "- **Dependencies**: If set to `False`, a valid `GENAI_API_KEY` must be configured.\n",
        "\n",
        "### `use_mock_search_engine`\n",
        "- **Purpose**: Controls whether the Google Custom Search fallback uses live API calls or mocked search results.\n",
        "- **Configuration**: Set to `True` for mock, `False` for live.\n",
        "- **Default**: `True`. Google Custom Search calls will be mocked.\n",
        "- **Dependencies**: If set to `False`, valid `SEARCH_API_KEY` and `SEARCH_CX` must be configured.\n",
        "\n",
        "## Advanced Settings\n",
        "\n",
        "### `chrome_options`\n",
        "- **Purpose**: These are advanced settings for the Selenium WebDriver (Chrome). They control browser behavior like running headless (without a visible UI).\n",
        "- **Configuration**: Modified directly in the \"User-configurable parameters\" cell.\n",
        "- **Default**: Pre-configured for headless operation, no-sandbox, and other common settings for server environments. It's generally not necessary to change these unless you have specific WebDriver requirements.\n",
        "- **Dependencies**: None."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1\n",
        "# Install Install necessary libraries\n",
        "!pip install supabase selenium webdriver-manager google-generativeai google-api-python-client tenacity"
      ],
      "metadata": {
        "id": "Uen98yUtKVl0",
        "outputId": "53e3fc9c-4bac-4100-98e7-cabd29c8d352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supabase\n",
            "  Downloading supabase-2.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.169.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Collecting gotrue<3.0.0,>=2.11.0 (from supabase)\n",
            "  Downloading gotrue-2.12.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Collecting postgrest<1.1,>0.19 (from supabase)\n",
            "  Downloading postgrest-1.0.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting realtime<2.5.0,>=2.4.0 (from supabase)\n",
            "  Downloading realtime-2.4.3-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting storage3<0.12,>=0.10 (from supabase)\n",
            "  Downloading storage3-0.11.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting supafunc<0.10,>=0.9 (from supabase)\n",
            "  Downloading supafunc-0.9.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Collecting pytest-mock<4.0.0,>=3.14.0 (from gotrue<3.0.0,>=2.11.0->supabase)\n",
            "  Downloading pytest_mock-3.14.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest<1.1,>0.19->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Collecting aiohttp<4.0.0,>=3.11.18 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading aiohttp-3.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (2.9.0.post0)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/websockets/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting websockets<15,>=11 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Collecting strenum<0.5.0,>=0.4.15 (from supafunc<0.10,>=0.9->supabase)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.20.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.5.0,>=2.4.0->supabase) (1.17.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.6.0)\n",
            "Downloading supabase-2.15.2-py3-none-any.whl (17 kB)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading gotrue-2.12.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading postgrest-1.0.2-py3-none-any.whl (22 kB)\n",
            "Downloading realtime-2.4.3-py3-none-any.whl (22 kB)\n",
            "Downloading storage3-0.11.3-py3-none-any.whl (17 kB)\n",
            "Downloading supafunc-0.9.4-py3-none-any.whl (7.8 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading aiohttp-3.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pytest_mock-3.14.1-py3-none-any.whl (9.9 kB)\n",
            "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: strenum, wsproto, websockets, python-dotenv, outcome, deprecation, webdriver-manager, trio, pytest-mock, aiohttp, trio-websocket, realtime, supafunc, storage3, selenium, postgrest, gotrue, supabase\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "Successfully installed aiohttp-3.12.4 deprecation-2.1.0 gotrue-2.12.0 outcome-1.3.0.post0 postgrest-1.0.2 pytest-mock-3.14.1 python-dotenv-1.1.0 realtime-2.4.3 selenium-4.33.0 storage3-0.11.3 strenum-0.4.15 supabase-2.15.2 supafunc-0.9.4 trio-0.30.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 websockets-14.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2\n",
        "# Chrome Installation for Google Colab\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def ensure_chrome_installed():\n",
        "    \"\"\"Ensures Chrome is installed in the Colab environment.\"\"\"\n",
        "    try:\n",
        "        # Check if Chrome is already available\n",
        "        result = subprocess.run(['which', 'google-chrome'],\n",
        "                              capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ Chrome is already installed and available.\")\n",
        "            return True\n",
        "\n",
        "        print(\"🔧 Chrome not found. Installing Chrome for Selenium...\")\n",
        "\n",
        "        # Install Chrome\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - > /dev/null 2>&1')\n",
        "        os.system('echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list')\n",
        "        os.system('apt-get update > /dev/null 2>&1')\n",
        "        os.system('apt-get install -y google-chrome-stable > /dev/null 2>&1')\n",
        "\n",
        "        # Verify installation\n",
        "        result = subprocess.run(['google-chrome', '--version'],\n",
        "                              capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"✅ Chrome installed successfully: {result.stdout.strip()}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ Chrome installation may have failed.\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during Chrome installation: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run the installation check\n",
        "chrome_ready = ensure_chrome_installed()\n",
        "if chrome_ready:\n",
        "    print(\"🚀 Ready to proceed with Selenium operations!\")\n",
        "else:\n",
        "    print(\"⚠️  You may need to restart the runtime if Chrome installation failed.\")"
      ],
      "metadata": {
        "id": "nh-iBdns2kxW",
        "outputId": "66ea360b-e5e4-4c54-9a38-a5c09af991ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Chrome not found. Installing Chrome for Selenium...\n",
            "✅ Chrome installed successfully: Google Chrome 137.0.7151.55\n",
            "🚀 Ready to proceed with Selenium operations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXNwVrnLxqHJ",
        "outputId": "e839047d-db23-400f-fea8-816ce149aa46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: chrome_options not found. It should be defined in the User-configurable parameters cell.\n"
          ]
        }
      ],
      "source": [
        "# Cell 3\n",
        "# Setup API Keys\n",
        "\n",
        "# Standard library imports\n",
        "import sqlite3\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Third-party library imports\n",
        "import requests # For simple HTTP requests (though less used now with Selenium)\n",
        "from bs4 import BeautifulSoup # For parsing HTML\n",
        "# from google.colab import userdata # Moved to User-configurable parameters cell\n",
        "\n",
        "# Selenium imports for web automation and dynamic content loading\n",
        "from selenium import webdriver\n",
        "# from selenium.webdriver.chrome.options import Options # Moved to User-configurable parameters cell\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
        "\n",
        "# Google GenAI imports (for Gemini model)\n",
        "# import google.generativeai as genai # Moved to User-configurable parameters cell\n",
        "from google.api_core.exceptions import DeadlineExceeded, ServiceUnavailable, ResourceExhausted, InternalServerError, GoogleAPIError\n",
        "\n",
        "# Google API Client imports (for Custom Search API)\n",
        "from googleapiclient.errors import HttpError\n",
        "# To use the live Google Custom Search API, uncomment the following import in this cell\n",
        "# AND in Cell where `build` is called.\n",
        "# from googleapiclient.discovery import build\n",
        "\n",
        "# Tenacity library for robust retry mechanisms\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError\n",
        "\n",
        "# --- Selenium WebDriver Setup ---\n",
        "# chrome_options is now defined in the first code cell (User-configurable parameters).\n",
        "# Ensure it's available in the global scope if defined in the first cell.\n",
        "if 'chrome_options' not in globals():\n",
        "    print(\"Error: chrome_options not found. It should be defined in the User-configurable parameters cell.\")\n",
        "    # Fallback to basic options if not found, though this indicates an issue with notebook structure\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "\n",
        "driver = None # Global WebDriver instance\n",
        "\n",
        "def setup_driver():\n",
        "    \"\"\"Initializes and returns the Selenium WebDriver instance.\"\"\"\n",
        "    global driver\n",
        "    if driver is None:\n",
        "        try:\n",
        "            print(\"Setting up Chrome WebDriver...\")\n",
        "            # ChromeDriver is automatically managed by webdriver_manager\n",
        "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "            print(\"WebDriver setup successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up WebDriver: {e}\")\n",
        "            print(\"Ensure Chrome is installed if not using a pre-built environment like Colab.\")\n",
        "            driver = None\n",
        "    return driver\n",
        "\n",
        "def close_driver():\n",
        "    \"\"\"Closes the Selenium WebDriver instance if it's active.\"\"\"\n",
        "    global driver\n",
        "    if driver:\n",
        "        print(\"Closing WebDriver...\")\n",
        "        driver.quit()\n",
        "        driver = None\n",
        "        print(\"WebDriver closed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "user-configurable-params",
        "outputId": "808d3ec6-1238-45be-a7b5-820cb2ae8079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- User Configurable Parameters Cell Initializing ---\n",
            "Processing will be limited to 1000 randomly selected dioceses.\n",
            "Supabase URL and Key loaded successfully.\n",
            "GenAI configured successfully for LIVE calls if relevant mock flags are False.\n",
            "Google Custom Search API Key and CX loaded. Ready for LIVE calls if use_mock_search_engine is False.\n",
            "Mocking settings: Direct Page GenAI=False, Snippet GenAI=False, Search Engine=False\n",
            "Chrome options configured.\n",
            "--- End User Configurable Parameters Cell ---\n"
          ]
        }
      ],
      "source": [
        "# Cell 4\n",
        "# User-configurable-parameters\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import os # For os.path.exists logic if adapted\n",
        "SUPABASE_URL_FROM_USERDATA = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_KEY_FROM_USERDATA = userdata.get('SUPABASE_KEY')\n",
        "\n",
        "print(\"--- User Configurable Parameters Cell Initializing ---\")\n",
        "\n",
        "# --- Processing Limit Configuration ---\n",
        "# Set the maximum number of random dioceses to process (None = process all)\n",
        "# This limits processing to help with testing or API quota management\n",
        "MAX_DIOCESES_TO_PROCESS = 1000  # Change this number or set to None to process all dioceses\n",
        "\n",
        "if MAX_DIOCESES_TO_PROCESS:\n",
        "    print(f\"Processing will be limited to {MAX_DIOCESES_TO_PROCESS} randomly selected dioceses.\")\n",
        "else:\n",
        "    print(\"Processing will include all dioceses that lack parish directory URLs.\")\n",
        "\n",
        "# --- Supabase Configuration ---\n",
        "SUPABASE_URL = None\n",
        "SUPABASE_KEY = None\n",
        "\n",
        "if SUPABASE_URL_FROM_USERDATA:\n",
        "    SUPABASE_URL = SUPABASE_URL_FROM_USERDATA\n",
        "if SUPABASE_KEY_FROM_USERDATA:\n",
        "    SUPABASE_KEY = SUPABASE_KEY_FROM_USERDATA\n",
        "\n",
        "if SUPABASE_URL and SUPABASE_KEY:\n",
        "    print(\"Supabase URL and Key loaded successfully.\")\n",
        "else:\n",
        "    print(\"Supabase URL and/or Key NOT loaded. Please check Colab Secrets.\")\n",
        "\n",
        "# --- GenAI API Key Setup ---\n",
        "# To use live GenAI calls:\n",
        "# 1. Ensure your GENAI_API_KEY_USCCB is stored in Colab Secrets.\n",
        "# 2. EITHER: Uncomment the line below that assigns GENAI_API_KEY_FROM_USERDATA to GENAI_API_KEY\n",
        "#    OR: Directly assign your key string to GENAI_API_KEY.\n",
        "# 3. Set the use_mock_genai_direct_page and use_mock_genai_snippet flags (defined below) to False.\n",
        "GENAI_API_KEY_FROM_USERDATA = userdata.get('GENAI_API_KEY_USCCB')\n",
        "GENAI_API_KEY = None # Default: No API key, forces mock.\n",
        "\n",
        "# UPDATED: Uncomment this line to use your API key from Colab Secrets\n",
        "if GENAI_API_KEY_FROM_USERDATA and GENAI_API_KEY_FROM_USERDATA not in [\"YOUR_API_KEY_PLACEHOLDER\", \"SET_YOUR_KEY_HERE\"]:\n",
        "    GENAI_API_KEY = GENAI_API_KEY_FROM_USERDATA # Now using key from Colab Secrets\n",
        "\n",
        "if GENAI_API_KEY:\n",
        "    try:\n",
        "        genai.configure(api_key=GENAI_API_KEY)\n",
        "        print(\"GenAI configured successfully for LIVE calls if relevant mock flags are False.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GenAI with key: {e}. GenAI features will be mocked.\")\n",
        "        GENAI_API_KEY = None # Ensure mock if configuration fails\n",
        "else:\n",
        "    print(\"GenAI API Key is not set. GenAI features will be mocked globally.\")\n",
        "\n",
        "# --- Search Engine API Key Setup ---\n",
        "# To use live Google Custom Search API calls:\n",
        "# 1. Ensure your SEARCH_API_KEY_USCCB and SEARCH_CX_USCCB are in Colab Secrets.\n",
        "# 2. EITHER: Uncomment the lines below that assign _FROM_USERDATA to SEARCH_API_KEY and SEARCH_CX\n",
        "#    OR: Directly assign your key strings.\n",
        "# 3. Set the use_mock_search_engine flag (defined below) to False.\n",
        "SEARCH_API_KEY_FROM_USERDATA = userdata.get('SEARCH_API_KEY_USCCB')\n",
        "SEARCH_CX_FROM_USERDATA = userdata.get('SEARCH_CX_USCCB')\n",
        "\n",
        "SEARCH_API_KEY = None # Default: No API key, forces mock.\n",
        "SEARCH_CX = None      # Default: No CX, forces mock.\n",
        "# UPDATED: Uncomment these lines to use your keys from Colab Secrets\n",
        "if SEARCH_API_KEY_FROM_USERDATA and SEARCH_API_KEY_FROM_USERDATA not in [\"YOUR_API_KEY_PLACEHOLDER\", \"SET_YOUR_KEY_HERE\"]:\n",
        "    SEARCH_API_KEY = SEARCH_API_KEY_FROM_USERDATA # Now using key from Colab Secrets\n",
        "if SEARCH_CX_FROM_USERDATA and SEARCH_CX_FROM_USERDATA not in [\"YOUR_CX_PLACEHOLDER\", \"SET_YOUR_CX_HERE\"]:\n",
        "    SEARCH_CX = SEARCH_CX_FROM_USERDATA            # Now using CX from Colab Secrets\n",
        "\n",
        "if SEARCH_API_KEY and SEARCH_CX:\n",
        "    print(\"Google Custom Search API Key and CX loaded. Ready for LIVE calls if use_mock_search_engine is False.\")\n",
        "else:\n",
        "    print(\"Google Custom Search API Key and/or CX are NOT configured or available. Search engine calls will be mocked.\")\n",
        "\n",
        "# --- Mocking Controls ---\n",
        "# These flags determine whether to use live API calls or mocked responses.\n",
        "# UPDATED: Set these to False to attempt LIVE API calls (since your APIs are now working)\n",
        "global use_mock_genai_direct_page\n",
        "use_mock_genai_direct_page = False  # Changed from True to False - Use LIVE GenAI for direct page analysis\n",
        "# Set to False to attempt LIVE GenAI calls for search snippet analysis (requires valid GENAI_API_KEY)\n",
        "global use_mock_genai_snippet\n",
        "use_mock_genai_snippet = False  # Changed from True to False - Use LIVE GenAI for snippet analysis\n",
        "# UPDATED: Set to False to attempt LIVE Google Custom Search calls (since your API is now working)\n",
        "global use_mock_search_engine\n",
        "use_mock_search_engine = False  # Changed from True to False - Use LIVE Google Custom Search\n",
        "\n",
        "print(f\"Mocking settings: Direct Page GenAI={use_mock_genai_direct_page}, Snippet GenAI={use_mock_genai_snippet}, Search Engine={use_mock_search_engine}\")\n",
        "\n",
        "# --- Selenium WebDriver Options ---\n",
        "global chrome_options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"window-size=1920,1080\")\n",
        "print(\"Chrome options configured.\")\n",
        "\n",
        "print(\"--- End User Configurable Parameters Cell ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xjtTXfgxwOq",
        "outputId": "e209b172-e570-404d-94a5-953e30cb8cc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supabase client initialized successfully.\n",
            "Fetched 196 total records from Dioceses table.\n",
            "Found 195 dioceses already processed with valid URLs in DiocesesParishDirectory.\n",
            "Found 1 dioceses needing parish directory URLs (or not yet in DiocesesParishDirectory).\n",
            "All 1 unprocessed dioceses will be processed (within limit of 1000).\n",
            "Prepared 1 dioceses for scanning.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5\n",
        "# Fetch Dioceses Info from Supabase database\n",
        "\n",
        "import random\n",
        "from supabase import create_client, Client\n",
        "import os # os is kept for MAX_DIOCESES_TO_PROCESS logic check from globals()\n",
        "\n",
        "# Initialize Supabase Client\n",
        "# These should be available from Cell 3 (User-configurable parameters)\n",
        "# SUPABASE_URL, SUPABASE_KEY\n",
        "if 'SUPABASE_URL' in globals() and 'SUPABASE_KEY' in globals() and SUPABASE_URL and SUPABASE_KEY:\n",
        "    try:\n",
        "        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "        print(\"Supabase client initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Supabase client: {e}\")\n",
        "        supabase = None\n",
        "else:\n",
        "    print(\"Error: Supabase URL or Key not found in global variables. Please ensure they are set in Cell 3.\")\n",
        "    supabase = None\n",
        "\n",
        "dioceses_to_scan = []\n",
        "if supabase:\n",
        "    try:\n",
        "        # Fetch all dioceses\n",
        "        response_dioceses = supabase.table('Dioceses').select('Website, Name').execute()\n",
        "        all_dioceses_list = response_dioceses.data if response_dioceses.data is not None else []\n",
        "        print(f\"Fetched {len(all_dioceses_list)} total records from Dioceses table.\")\n",
        "\n",
        "        # Fetch dioceses that already have a valid parish directory URL\n",
        "        response_processed_dioceses = supabase.table('DiocesesParishDirectory').select('diocese_url').not_.is_('parish_directory_url', 'null').not_.eq('parish_directory_url', '').execute()\n",
        "\n",
        "        processed_diocese_urls = {item['diocese_url'] for item in response_processed_dioceses.data} if response_processed_dioceses.data is not None else set()\n",
        "        print(f\"Found {len(processed_diocese_urls)} dioceses already processed with valid URLs in DiocesesParishDirectory.\")\n",
        "\n",
        "        # Filter out processed dioceses\n",
        "        unprocessed_dioceses = [\n",
        "            {'url': d['Website'], 'name': d['Name']}\n",
        "            for d in all_dioceses_list\n",
        "            if d['Website'] not in processed_diocese_urls\n",
        "        ]\n",
        "        print(f\"Found {len(unprocessed_dioceses)} dioceses needing parish directory URLs (or not yet in DiocesesParishDirectory).\")\n",
        "\n",
        "        # Apply limit if MAX_DIOCESES_TO_PROCESS is set (ensure MAX_DIOCESES_TO_PROCESS is accessible)\n",
        "        # MAX_DIOCESES_TO_PROCESS should be defined in Cell 3.\n",
        "        if 'MAX_DIOCESES_TO_PROCESS' in globals() and MAX_DIOCESES_TO_PROCESS is not None:\n",
        "            if len(unprocessed_dioceses) > MAX_DIOCESES_TO_PROCESS:\n",
        "                dioceses_to_scan = random.sample(unprocessed_dioceses, MAX_DIOCESES_TO_PROCESS)\n",
        "                print(f\"Randomly selected {len(dioceses_to_scan)} dioceses for processing (limit: {MAX_DIOCESES_TO_PROCESS}).\")\n",
        "            else:\n",
        "                dioceses_to_scan = unprocessed_dioceses\n",
        "                print(f\"All {len(dioceses_to_scan)} unprocessed dioceses will be processed (within limit of {MAX_DIOCESES_TO_PROCESS}).\")\n",
        "        else:\n",
        "            dioceses_to_scan = unprocessed_dioceses\n",
        "            print(f\"All {len(dioceses_to_scan)} unprocessed dioceses will be processed (no limit set).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Supabase data operations: {e}\")\n",
        "        dioceses_to_scan = [] # Ensure it's empty on error\n",
        "else:\n",
        "    print(\"Supabase client not initialized. Skipping data fetch.\")\n",
        "    dioceses_to_scan = []\n",
        "\n",
        "# Final check and message\n",
        "if not dioceses_to_scan:\n",
        "    print(\"No dioceses to scan based on Supabase data and MAX_DIOCESES_TO_PROCESS setting.\")\n",
        "else:\n",
        "    print(f\"Prepared {len(dioceses_to_scan)} dioceses for scanning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B50-0qSsxyhH"
      },
      "outputs": [],
      "source": [
        "# Cell 6\n",
        "# Function to find candidate parish listing URLs from page content\n",
        "\n",
        "from urllib.parse import urljoin, urlparse # For handling relative and absolute URLs\n",
        "import re # For regular expression matching in URL paths\n",
        "\n",
        "def normalize_url_join(base_url, relative_url):\n",
        "    \"\"\"Properly joins URLs while avoiding double slashes.\"\"\"\n",
        "    # Remove trailing slash from base_url if relative_url starts with slash\n",
        "    if base_url.endswith('/') and relative_url.startswith('/'):\n",
        "        base_url = base_url.rstrip('/')\n",
        "    return urljoin(base_url, relative_url)\n",
        "\n",
        "def get_surrounding_text(element, max_length=200):\n",
        "    \"\"\"Extracts text from the parent element of a given link, limited in length.\n",
        "    This provides context for the link.\n",
        "    \"\"\"\n",
        "    if element and element.parent:\n",
        "        parent_text = element.parent.get_text(separator=' ', strip=True)\n",
        "        # Truncate if too long to keep prompts for GenAI concise\n",
        "        return parent_text[:max_length] + ('...' if len(parent_text) > max_length else '')\n",
        "    return ''\n",
        "\n",
        "def find_candidate_urls(soup, base_url):\n",
        "    \"\"\"Scans a BeautifulSoup soup object for potential parish directory links.\n",
        "    It uses a combination of keyword matching in link text/surrounding text\n",
        "    and regex patterns for URL paths.\n",
        "    Returns a list of candidate link dictionaries.\n",
        "    \"\"\"\n",
        "    candidate_links = []\n",
        "    processed_hrefs = set() # To avoid adding duplicate URLs\n",
        "\n",
        "    # Keywords likely to appear in link text or surrounding text for parish directories\n",
        "    parish_link_keywords = [\n",
        "        'Churches', 'Directory of Parishes', 'Parishes', 'parishfinder', 'Parish Finder',\n",
        "        'Find a Parish', 'Locations', 'Our Parishes', 'Parish Listings', 'Find a Church',\n",
        "        'Church Directory', 'Faith Communities', 'Find Mass Times', 'Our Churches',\n",
        "        'Search Parishes', 'Parish Map', 'Mass Schedule', 'Sacraments', 'Worship'\n",
        "    ]\n",
        "    # Regex patterns for URL paths that often indicate a parish directory\n",
        "    url_patterns = [\n",
        "        r'parishes', r'directory', r'locations', r'churches',\n",
        "        r'parish-finder', r'findachurch', r'parishsearch', r'parishdirectory',\n",
        "        r'find-a-church', r'church-directory', r'parish-listings', r'parish-map',\n",
        "        r'mass-times', r'sacraments', r'search', r'worship', r'finder'\n",
        "    ]\n",
        "\n",
        "    all_links_tags = soup.find_all('a', href=True) # Find all <a> tags with an href attribute\n",
        "\n",
        "    for link_tag in all_links_tags:\n",
        "        href = link_tag['href']\n",
        "        # Skip empty, anchor, JavaScript, or mailto links\n",
        "        if not href or href.startswith('#') or href.lower().startswith('javascript:') or href.lower().startswith('mailto:'):\n",
        "            continue\n",
        "\n",
        "        abs_href = normalize_url_join(base_url, href) # Resolve relative URLs to absolute with fixed joining\n",
        "        if not abs_href.startswith('http'): # Ensure it's a web link\n",
        "            continue\n",
        "        if abs_href in processed_hrefs: # Avoid re-processing the same URL\n",
        "            continue\n",
        "\n",
        "        link_text = link_tag.get_text(strip=True)\n",
        "        surrounding_text = get_surrounding_text(link_tag)\n",
        "        parsed_href_path = urlparse(abs_href).path.lower() # Get the path component of the URL\n",
        "\n",
        "        # Check for matches based on keywords in text or URL patterns\n",
        "        text_match = any(keyword.lower() in link_text.lower() or keyword.lower() in surrounding_text.lower() for keyword in parish_link_keywords)\n",
        "        pattern_match = any(re.search(pattern, parsed_href_path, re.IGNORECASE) for pattern in url_patterns)\n",
        "\n",
        "        if text_match or pattern_match:\n",
        "            candidate_links.append({\n",
        "                'text': link_text,\n",
        "                'href': abs_href,\n",
        "                'surrounding_text': surrounding_text\n",
        "            })\n",
        "            processed_hrefs.add(abs_href)\n",
        "\n",
        "    return candidate_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen_ai_analyzer_cell_NEW"
      },
      "outputs": [],
      "source": [
        "# Cell 7\n",
        "# GenAI Powered Link Analyzer (for direct page content)\n",
        "\n",
        "# Define exceptions on which GenAI calls should be retried\n",
        "RETRYABLE_GENAI_EXCEPTIONS = (\n",
        "    DeadlineExceeded, ServiceUnavailable, ResourceExhausted,\n",
        "    InternalServerError, GoogleAPIError\n",
        ")\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3), # Retry up to 3 times\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=10), # Exponential backoff: 2s, 4s, 8s...\n",
        "    retry=retry_if_exception_type(RETRYABLE_GENAI_EXCEPTIONS),\n",
        "    reraise=True # Reraise the last exception if all retries fail\n",
        ")\n",
        "def _invoke_genai_model_with_retry(prompt):\n",
        "    \"\"\"Internal helper to invoke the GenAI model with retry logic.\"\"\"\n",
        "    # print(\"    Attempting GenAI call...\") # Uncomment for debugging retries\n",
        "    # GENAI_API_KEY is configured in the first cell. If None, this will fail if not mocked.\n",
        "    # Ensure genai is available if first cell wasn't run, or handle error\n",
        "    if 'genai' not in globals():\n",
        "        raise NameError(\"genai module not available. Ensure User-configurable parameters cell is run.\")\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Or your preferred model\n",
        "    return model.generate_content(prompt)\n",
        "\n",
        "def analyze_links_with_genai(candidate_links, diocese_name=None):\n",
        "    \"\"\"Analyzes candidate links using GenAI (or mock) to find the best parish directory URL.\"\"\"\n",
        "    best_link_found = None\n",
        "    highest_score = -1\n",
        "\n",
        "    # --- Mock vs. Live Control for GenAI (Direct Page Analysis) ---\n",
        "    # Control for this is `use_mock_genai_direct_page` from the User-configurable parameters cell.\n",
        "    # GENAI_API_KEY is also defined there.\n",
        "    # Ensure mock if key is not configured, overriding user setting for safety.\n",
        "    current_use_mock_direct = use_mock_genai_direct_page if ('GENAI_API_KEY' in globals() and GENAI_API_KEY) else True\n",
        "\n",
        "    if not current_use_mock_direct:\n",
        "        print(f\"Attempting LIVE GenAI analysis for {len(candidate_links)} direct page links for {diocese_name or 'Unknown Diocese'}.\")\n",
        "    # else:\n",
        "        # print(f\"Using MOCKED GenAI analysis for {len(candidate_links)} direct page links for {diocese_name or 'Unknown Diocese'}.\")\n",
        "    # ---\n",
        "\n",
        "    if current_use_mock_direct:\n",
        "        mock_keywords = ['parish', 'church', 'directory', 'location', 'finder', 'search', 'map', 'listing', 'sacrament', 'mass', 'worship']\n",
        "        for link_info in candidate_links:\n",
        "            current_score = 0\n",
        "            text_to_check = (link_info['text'] + ' ' + link_info['href'] + ' ' + link_info['surrounding_text']).lower()\n",
        "            for kw in mock_keywords:\n",
        "                if kw in text_to_check: current_score += 3\n",
        "            if diocese_name and diocese_name.lower() in text_to_check: current_score +=1\n",
        "            current_score = min(current_score, 10) # Cap score at 10\n",
        "            if current_score >= 7 and current_score > highest_score: # Threshold of 7\n",
        "                highest_score = current_score\n",
        "                best_link_found = link_info['href']\n",
        "        return best_link_found\n",
        "\n",
        "    # --- Actual GenAI API Call Logic (executes if use_mock is False) ---\n",
        "    for link_info in candidate_links:\n",
        "        prompt = f\"\"\"Given the following information about a link from the {diocese_name or 'a diocesan'} website:\n",
        "        Link Text: \"{link_info['text']}\"\n",
        "        Link URL: \"{link_info['href']}\"\n",
        "        Surrounding Text: \"{link_info['surrounding_text']}\"\n",
        "        Does this link likely lead to a parish directory, a list of churches, or a way to find parishes?\n",
        "        Respond with a confidence score from 0 (not likely) to 10 (very likely) and a brief justification.\n",
        "        Format as: Score: [score], Justification: [text]\"\"\"\n",
        "        try:\n",
        "            response = _invoke_genai_model_with_retry(prompt)\n",
        "            response_text = response.text\n",
        "            # print(f\"    GenAI Raw Response (Direct Link): {response_text}\") # For debugging\n",
        "            score_match = re.search(r\"Score: (\\d+)\", response_text, re.IGNORECASE)\n",
        "            if score_match:\n",
        "                score = int(score_match.group(1))\n",
        "                if score >= 7 and score > highest_score:\n",
        "                    highest_score = score\n",
        "                    best_link_found = link_info['href']\n",
        "            # else: print(f\"    Could not parse score from GenAI (Direct Link) for {link_info['href']}: {response_text}\")\n",
        "        except RetryError as e:\n",
        "            print(f\"    GenAI API call (Direct Link) failed after multiple retries for {link_info['href']}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"    Error calling GenAI (Direct Link) for {link_info['href']}: {e}. No score assigned.\")\n",
        "    return best_link_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "search_engine_fallback_cell_NEW"
      },
      "outputs": [],
      "source": [
        "# Cell 8\n",
        "# Search Engine Fallback Functions & GenAI Snippet Analysis\n",
        "\n",
        "# Ensure 'build' is imported if using live search. It's commented in Cell 1 by default.\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "def is_retryable_http_error(exception):\n",
        "    \"\"\"Custom retry condition for HttpError: only retry on 5xx or 429 (rate limit).\"\"\"\n",
        "    if isinstance(exception, HttpError):\n",
        "        return exception.resp.status >= 500 or exception.resp.status == 429\n",
        "    return False\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
        "    retry=retry_if_exception_type(HttpError),\n",
        "    reraise=True\n",
        ")\n",
        "def _invoke_search_api_with_retry(service, query, cx_id):\n",
        "    \"\"\"Internal helper to invoke the Google Custom Search API with retry logic.\"\"\"\n",
        "    # print(f\"    Attempting Search API call for query: {query}\") # Uncomment for debugging retries\n",
        "    return service.cse().list(q=query, cx=cx_id, num=3).execute() # Fetch top 3 results per query\n",
        "\n",
        "def normalize_mock_url(base_url, path):\n",
        "    \"\"\"Properly constructs URLs for mock data, avoiding double slashes.\"\"\"\n",
        "    # Ensure base_url doesn't end with slash and path starts with slash\n",
        "    base_clean = base_url.rstrip('/')\n",
        "    path_clean = path if path.startswith('/') else '/' + path\n",
        "    return base_clean + path_clean\n",
        "\n",
        "def analyze_search_snippet_with_genai(search_results, diocese_name):\n",
        "    \"\"\"Analyzes search result snippets using GenAI (or mock) to find the best parish directory URL.\"\"\"\n",
        "    best_link_from_snippet = None\n",
        "    highest_score = -1\n",
        "\n",
        "    # --- Mock vs. Live Control for GenAI (Snippet Analysis) ---\n",
        "    # Control for this is `use_mock_genai_snippet` from the User-configurable parameters cell.\n",
        "    # GENAI_API_KEY is also defined there.\n",
        "    # Ensure mock if key is not configured, overriding user setting for safety.\n",
        "    current_use_mock_snippet = use_mock_genai_snippet if ('GENAI_API_KEY' in globals() and GENAI_API_KEY) else True\n",
        "\n",
        "    if not current_use_mock_snippet:\n",
        "        print(f\"Attempting LIVE GenAI analysis for {len(search_results)} snippets for {diocese_name}.\")\n",
        "    # else:\n",
        "        # print(f\"Using MOCKED GenAI analysis for {len(search_results)} snippets for {diocese_name}.\")\n",
        "    # ---\n",
        "\n",
        "    if current_use_mock_snippet:\n",
        "        mock_keywords = ['parish', 'church', 'directory', 'location', 'finder', 'search', 'map', 'listing', 'mass times']\n",
        "        for result in search_results:\n",
        "            current_score = 0\n",
        "            text_to_check = (result.get('title', '') + ' ' + result.get('snippet', '') + ' ' + result.get('link', '')).lower()\n",
        "            for kw in mock_keywords:\n",
        "                if kw in text_to_check: current_score += 3\n",
        "            if diocese_name and diocese_name.lower() in text_to_check: current_score += 1\n",
        "            current_score = min(current_score, 10)\n",
        "            if current_score >= 7 and current_score > highest_score: # Threshold of 7\n",
        "                highest_score = current_score\n",
        "                best_link_from_snippet = result.get('link')\n",
        "        return best_link_from_snippet\n",
        "\n",
        "    # --- Actual GenAI API Call Logic for Snippets (executes if use_mock_genai_for_snippet is False) ---\n",
        "    for result in search_results:\n",
        "        title = result.get('title', '')\n",
        "        snippet = result.get('snippet', '')\n",
        "        link = result.get('link', '')\n",
        "        prompt = f\"\"\"Given the following search result from {diocese_name}'s website:\n",
        "        Title: \"{title}\"\n",
        "        Snippet: \"{snippet}\"\n",
        "        URL: \"{link}\"\n",
        "        Does this link likely lead to a parish directory, church locator, or list of churches?\n",
        "        Respond with a confidence score from 0 (not likely) to 10 (very likely) and a brief justification.\n",
        "        Format as: Score: [score], Justification: [text]\"\"\"\n",
        "        try:\n",
        "            # Uses the same _invoke_genai_model_with_retry as direct page analysis\n",
        "            response = _invoke_genai_model_with_retry(prompt)\n",
        "            response_text = response.text\n",
        "            # print(f\"    GenAI Raw Response (Snippet): {response_text}\") # For debugging\n",
        "            score_match = re.search(r\"Score: (\\d+)\", response_text, re.IGNORECASE)\n",
        "            if score_match:\n",
        "                score = int(score_match.group(1))\n",
        "                if score >= 7 and score > highest_score:\n",
        "                    highest_score = score\n",
        "                    best_link_from_snippet = link\n",
        "            # else: print(f\"    Could not parse score from GenAI (Snippet) for {link}: {response_text}\")\n",
        "        except RetryError as e:\n",
        "            print(f\"    GenAI API call (Snippet) for {link} failed after multiple retries: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"    Error calling GenAI for snippet analysis of {link}: {e}\")\n",
        "    return best_link_from_snippet\n",
        "\n",
        "def search_for_directory_link(diocese_name, diocese_website_url):\n",
        "    \"\"\"Uses Google Custom Search (or mock) to find potential directory links, then analyzes snippets.\"\"\"\n",
        "    # print(f\"Executing search engine fallback for {diocese_name} ({diocese_website_url})\") # Verbose\n",
        "\n",
        "    # --- Mock vs. Live Control for Search Engine ---\n",
        "    # Control for this is `use_mock_search_engine` from the User-configurable parameters cell.\n",
        "    # SEARCH_API_KEY and SEARCH_CX are also defined there.\n",
        "    # Ensure mock if keys are not configured, overriding user setting for safety.\n",
        "    current_use_mock_search = use_mock_search_engine if ('SEARCH_API_KEY' in globals() and SEARCH_API_KEY and 'SEARCH_CX' in globals() and SEARCH_CX) else True\n",
        "\n",
        "    if not current_use_mock_search:\n",
        "        print(f\"Attempting LIVE Google Custom Search for {diocese_name}.\")\n",
        "    # else:\n",
        "        # print(f\"Using MOCKED Google Custom Search for {diocese_name}.\")\n",
        "    # ---\n",
        "\n",
        "    if current_use_mock_search:\n",
        "        mock_results = [\n",
        "            {'link': normalize_mock_url(diocese_website_url, '/parishes'), 'title': f\"Parishes - {diocese_name}\", 'snippet': f\"List of parishes in the Diocese of {diocese_name}. Find a parish near you.\"},\n",
        "            {'link': normalize_mock_url(diocese_website_url, '/directory'), 'title': f\"Directory - {diocese_name}\", 'snippet': f\"Official directory of churches and schools for {diocese_name}.\"},\n",
        "            {'link': normalize_mock_url(diocese_website_url, '/find-a-church'), 'title': f\"Find a Church - {diocese_name}\", 'snippet': f\"Search for a Catholic church in {diocese_name}. Mass times and locations.\"}\n",
        "        ]\n",
        "        # Simulate `site:` search by filtering mock results to the diocese's website\n",
        "        filtered_mock_results = [res for res in mock_results if res['link'].startswith(diocese_website_url.rstrip('/'))]\n",
        "        return analyze_search_snippet_with_genai(filtered_mock_results, diocese_name)\n",
        "\n",
        "    # --- Actual Google Custom Search API Call Logic (executes if use_mock_search is False) ---\n",
        "    try:\n",
        "        # `build` is imported at the top of this cell for clarity when live calls are made.\n",
        "        service = build(\"customsearch\", \"v1\", developerKey=SEARCH_API_KEY)\n",
        "        # Construct multiple queries to increase chances of finding the directory\n",
        "        queries = [\n",
        "            f\"parish directory site:{diocese_website_url}\",\n",
        "            f\"list of churches site:{diocese_website_url}\",\n",
        "            f\"find a parish site:{diocese_website_url}\",\n",
        "            f\"{diocese_name} parish directory\"  # Broader query without site restriction as a last resort\n",
        "        ]\n",
        "        search_results_items = []\n",
        "        unique_links = set()  # To avoid duplicate results from different queries\n",
        "\n",
        "        for q in queries:\n",
        "            if len(search_results_items) >= 5: break  # Limit total API calls/results\n",
        "            print(f\"    Executing search query: {q}\")\n",
        "            try:\n",
        "                # Use the retry-enabled helper for the API call\n",
        "                response = _invoke_search_api_with_retry(service, q, SEARCH_CX)\n",
        "                res_items = response.get('items', [])\n",
        "                for item in res_items:\n",
        "                    link = item.get('link')\n",
        "                    if link and link not in unique_links:\n",
        "                        search_results_items.append(item)\n",
        "                        unique_links.add(link)\n",
        "                time.sleep(0.2)  # Brief pause between queries to be polite to the API\n",
        "            except RetryError as e:\n",
        "                print(f\"    Search API call failed after retries for query '{q}': {e}\")\n",
        "                continue  # Try next query\n",
        "            except HttpError as e:\n",
        "                if e.resp.status == 403:\n",
        "                    print(f\"    Access denied (403) for query '{q}': {e.reason}\")\n",
        "                    print(\"    Check that Custom Search API is enabled and credentials are correct.\")\n",
        "                    break  # Stop trying other queries if we have auth issues\n",
        "                else:\n",
        "                    print(f\"    HTTP error for query '{q}': {e}\")\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"    Unexpected error for query '{q}': {e}\")\n",
        "                continue\n",
        "\n",
        "        if not search_results_items:\n",
        "            print(f\"    Search engine returned no results for {diocese_name}.\")\n",
        "            return None\n",
        "\n",
        "        # Format results for the snippet analyzer\n",
        "        formatted_results = [{'link': item.get('link'), 'title': item.get('title'), 'snippet': item.get('snippet')} for item in search_results_items]\n",
        "        return analyze_search_snippet_with_genai(formatted_results, diocese_name)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Error during search engine setup for {diocese_name}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMcje622x0Cn",
        "outputId": "013e7c7e-d9d6-4aec-b2f9-528f11b70e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No authenticated user found. This may cause RLS policy violations.\n",
            "Setting up Chrome WebDriver...\n",
            "WebDriver setup successfully.\n",
            "Processing 1 dioceses with Selenium...\n",
            "--- Processing: http://www.stamforddio.org/ (Ukrainian Catholic Eparchy of Stamford) ---\n",
            "    Result: General error processing http://www.stamforddio.org/: HTTPConnectionPool(host='localhost', port=37177): Read timed out. (read timeout=120)\n",
            "Closing WebDriver...\n",
            "WebDriver closed.\n"
          ]
        }
      ],
      "source": [
        "# Cell 9\n",
        "# Process URLs, Apply Analysis Stages, and Write Results to Supabase Database\n",
        "\n",
        "# Imports are assumed to be handled by previous cells (esp. for TimeoutException, WebDriverException, BeautifulSoup, time, retry)\n",
        "# from supabase import Client # Only if supabase client needs re-init and not global\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
        "    retry=retry_if_exception_type((TimeoutException, WebDriverException)),\n",
        "    reraise=True\n",
        ")\n",
        "def get_page_with_retry(driver_instance, url):\n",
        "    \"\"\"Wraps driver.get() with retry logic.\"\"\"\n",
        "    # print(f\"    Attempting to load page: {url}\") # Uncomment for debugging retries\n",
        "    driver_instance.get(url)\n",
        "\n",
        "# Check for Supabase client (initialized in Cell 5)\n",
        "if 'supabase' not in globals() or not supabase:\n",
        "    print(\"Error: Supabase client not found or not initialized. Please ensure Cell 5 (Supabase setup) ran correctly.\")\n",
        "    # If dioceses_to_scan was populated by Cell 5, it might still try to run, so clear it.\n",
        "    dioceses_to_scan = []\n",
        "\n",
        "# Debug: Check if user is authenticated\n",
        "if supabase:\n",
        "    try:\n",
        "        # Test authentication by trying to get user info\n",
        "        user = supabase.auth.get_user()\n",
        "        if user and user.user:\n",
        "            print(f\"Authenticated as user: {user.user.email}\")\n",
        "        else:\n",
        "            print(\"Warning: No authenticated user found. This may cause RLS policy violations.\")\n",
        "    except Exception as auth_error:\n",
        "        print(f\"Authentication check failed: {auth_error}\")\n",
        "\n",
        "if 'dioceses_to_scan' in locals() and dioceses_to_scan:\n",
        "    driver_instance = setup_driver() # Initialize the WebDriver\n",
        "    if driver_instance:\n",
        "        print(f\"Processing {len(dioceses_to_scan)} dioceses with Selenium...\")\n",
        "        for diocese_info in dioceses_to_scan:\n",
        "            current_url = diocese_info['url']\n",
        "            diocese_name = diocese_info['name']\n",
        "            print(f\"--- Processing: {current_url} ({diocese_name}) ---\")\n",
        "\n",
        "            parish_dir_url_found = None\n",
        "            status_text = \"Not Found\" # Default status\n",
        "            method = \"not_found_all_stages\" # Default method\n",
        "\n",
        "            try:\n",
        "                # Stage 1: Load page with Selenium\n",
        "                get_page_with_retry(driver_instance, current_url)\n",
        "                time.sleep(0.5)\n",
        "                page_source = driver_instance.page_source\n",
        "                soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "                # Stage 2: Find candidate links\n",
        "                candidate_links = find_candidate_urls(soup, current_url)\n",
        "\n",
        "                if candidate_links:\n",
        "                    # Stage 3: Analyze with GenAI\n",
        "                    print(f\"    Found {len(candidate_links)} candidates from direct page. Analyzing...\")\n",
        "                    parish_dir_url_found = analyze_links_with_genai(candidate_links, diocese_name)\n",
        "                    if parish_dir_url_found:\n",
        "                        method = \"genai_direct_page_analysis\"\n",
        "                        status_text = \"Success\"\n",
        "                    else:\n",
        "                        print(f\"    GenAI (direct page) did not find a suitable URL for {current_url}.\")\n",
        "                else:\n",
        "                    print(f\"    No candidate links found by direct page scan for {current_url}.\")\n",
        "\n",
        "                # Stage 4: Search engine fallback\n",
        "                if not parish_dir_url_found:\n",
        "                    print(f\"    Direct page analysis failed for {current_url}. Trying search engine fallback...\")\n",
        "                    parish_dir_url_found = search_for_directory_link(diocese_name, current_url)\n",
        "                    if parish_dir_url_found:\n",
        "                        method = \"search_engine_snippet_genai\"\n",
        "                        status_text = \"Success\"\n",
        "                    else:\n",
        "                        print(f\"    Search engine fallback also failed for {current_url}.\")\n",
        "\n",
        "                # Log final result for this diocese before DB write\n",
        "                if parish_dir_url_found:\n",
        "                     print(f\"    Result: Parish Directory URL for {current_url}: {parish_dir_url_found} (Method: {method})\")\n",
        "                else:\n",
        "                     print(f\"    Result: No Parish Directory URL definitively found for {current_url} (Final method: {method})\")\n",
        "\n",
        "                # Write to Supabase\n",
        "                if supabase: # Check if client is available\n",
        "                    data_to_upsert = {\n",
        "                        'diocese_url': current_url,\n",
        "                        'parish_directory_url': parish_dir_url_found,\n",
        "                        'found': status_text,\n",
        "                        'found_method': method\n",
        "                    }\n",
        "\n",
        "                    print(f\"    Attempting to upsert data: {data_to_upsert}\")\n",
        "\n",
        "                    try:\n",
        "                        # First, try to check if RLS is causing issues by testing table access\n",
        "                        test_response = supabase.table('DiocesesParishDirectory').select('*').limit(1).execute()\n",
        "                        print(f\"    Table access test successful, found {len(test_response.data)} rows\")\n",
        "\n",
        "                        # Now attempt the upsert\n",
        "                        response = supabase.table('DiocesesParishDirectory').upsert(data_to_upsert).execute()\n",
        "\n",
        "                        # Check for errors in the response object\n",
        "                        if hasattr(response, 'error') and response.error:\n",
        "                            error_detail = response.error.message if hasattr(response.error, 'message') else str(response.error)\n",
        "                            raise Exception(f\"Supabase upsert error: {error_detail}\")\n",
        "\n",
        "                        print(f\"    Successfully upserted data for {current_url} to Supabase.\")\n",
        "\n",
        "                    except Exception as supa_error:\n",
        "                        error_str = str(supa_error)\n",
        "                        print(f\"    Error upserting data to Supabase for {current_url}: {error_str}\")\n",
        "\n",
        "                        # Check if it's an RLS policy violation\n",
        "                        if '42501' in error_str or 'row-level security policy' in error_str.lower():\n",
        "                            print(\"    RLS Policy Issue Detected!\")\n",
        "                            print(\"    Solutions:\")\n",
        "                            print(\"    1. Authenticate with supabase.auth.sign_in_with_password(email, password)\")\n",
        "                            print(\"    2. Disable RLS: ALTER TABLE DiocesesParishDirectory DISABLE ROW LEVEL SECURITY;\")\n",
        "                            print(\"    3. Create a policy: CREATE POLICY ... ON DiocesesParishDirectory FOR ALL USING (true);\")\n",
        "\n",
        "                            # Try inserting without upsert as a fallback\n",
        "                            try:\n",
        "                                print(\"    Attempting regular insert as fallback...\")\n",
        "                                insert_response = supabase.table('DiocesesParishDirectory').insert(data_to_upsert).execute()\n",
        "                                if hasattr(insert_response, 'error') and insert_response.error:\n",
        "                                    print(f\"    Insert also failed: {insert_response.error}\")\n",
        "                                else:\n",
        "                                    print(\"    Insert succeeded!\")\n",
        "                            except Exception as insert_error:\n",
        "                                print(f\"    Insert fallback also failed: {insert_error}\")\n",
        "                else:\n",
        "                    print(f\"    Supabase client not available. Skipping database write for {current_url}.\")\n",
        "\n",
        "            except RetryError as e:\n",
        "                error_message = str(e).replace('\"', \"''\")\n",
        "                print(f\"    Result: Page load failed after multiple retries for {current_url}: {error_message[:100]}\")\n",
        "                status_text = f\"Error: Page load failed - {error_message[:60]}\"\n",
        "                method = \"error_page_load_failed\"\n",
        "                # Write error to Supabase\n",
        "                if supabase:\n",
        "                    data_to_upsert = {\n",
        "                        'diocese_url': current_url,\n",
        "                        'parish_directory_url': None,\n",
        "                        'found': status_text,\n",
        "                        'found_method': method\n",
        "                    }\n",
        "                    try:\n",
        "                        response = supabase.table('DiocesesParishDirectory').upsert(data_to_upsert).execute()\n",
        "                        if hasattr(response, 'error') and response.error:\n",
        "                            error_detail = response.error.message if hasattr(response.error, 'message') else str(response.error)\n",
        "                            raise Exception(f\"Supabase upsert error (on page load fail): {error_detail}\")\n",
        "                    except Exception as supa_error:\n",
        "                        print(f\"    Error upserting error data to Supabase for {current_url}: {supa_error}\")\n",
        "                else:\n",
        "                    print(f\"    Supabase client not available. Skipping database write for error on {current_url}.\")\n",
        "            except Exception as e:\n",
        "                error_message = str(e).replace('\"', \"''\")\n",
        "                print(f\"    Result: General error processing {current_url}: {error_message[:100]}\")\n",
        "                status_text = f\"Error: {error_message[:100]}\"\n",
        "                method = \"error_processing_general\"\n",
        "                # Write error to Supabase\n",
        "                if supabase:\n",
        "                    data_to_upsert = {\n",
        "                        'diocese_url': current_url,\n",
        "                        'parish_directory_url': None,\n",
        "                        'found': status_text,\n",
        "                        'found_method': method\n",
        "                    }\n",
        "                    try:\n",
        "                        response = supabase.table('DiocesesParishDirectory').upsert(data_to_upsert).execute()\n",
        "                        if hasattr(response, 'error') and response.error:\n",
        "                            error_detail = response.error.message if hasattr(response.error, 'message') else str(response.error)\n",
        "                            raise Exception(f\"Supabase upsert error (on general error): {error_detail}\")\n",
        "                    except Exception as supa_error:\n",
        "                        print(f\"    Error upserting error data to Supabase for {current_url}: {supa_error}\")\n",
        "                else:\n",
        "                    print(f\"    Supabase client not available. Skipping database write for error on {current_url}.\")\n",
        "\n",
        "        close_driver()\n",
        "    else:\n",
        "        print(\"Selenium WebDriver not available. Skipping URL processing.\")\n",
        "else:\n",
        "    print(\"No dioceses to scan (dioceses_to_scan is empty or not defined). Ensure Cell 5 (Supabase data fetch) ran correctly.\")"
      ]
    }
  ]
}